{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGStream Complete Tutorial with Schema Registry\n",
    "\n",
    "This interactive tutorial demonstrates the complete AGStream workflow with Karapace Schema Registry.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, make sure you have:\n",
    "1. Docker running with Kafka and Karapace services\n",
    "2. Python environment with agentics installed\n",
    "3. Basic understanding of Pydantic models\n",
    "\n",
    "```bash\n",
    "# Start services (if not already running)\n",
    "./manage_services.sh start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from agentics.core.streaming import AGStream\n",
    "from agentics.core.streaming_utils import (\n",
    "    register_atype_schema,\n",
    "    get_atype_from_registry,\n",
    "    list_schema_versions,\n",
    "    create_kafka_topic,\n",
    "    kafka_topic_exists,\n",
    ")\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# Set up logging for better visibility\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global queue for communication between threads\n",
    "message_queue = queue.Queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Pydantic Models\n",
    "\n",
    "Let's create some Pydantic models that we'll use throughout this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Question model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    \"\"\"A question to be answered by our agent\"\"\"\n",
    "    text: str = Field(description=\"The question text\")\n",
    "    category: str = Field(description=\"Question category\")\n",
    "    priority: int = Field(default=1, description=\"Priority level (1-5)\")\n",
    "\n",
    "# Define an Answer model\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"An answer to a question\"\"\"\n",
    "    text: str = Field(description=\"The answer text\")\n",
    "    confidence: float = Field(description=\"Confidence score (0-1)\")\n",
    "    sources: list[str] = Field(default_factory=list, description=\"Source references\")\n",
    "\n",
    "# Display our models\n",
    "print(\"Question model fields:\", list(Question.model_fields.keys()))\n",
    "print(\"Answer model fields:\", list(Answer.model_fields.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize AGStream with Schema Registry\n",
    "\n",
    "Let's create AGStream instances and connect them to the schema registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create AGStream for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_stream = AGStream(\n",
    "    atype=Question,\n",
    "    input_topic=\"tutorial-questions\",\n",
    "    output_topic=\"tutorial-answers\",\n",
    "    schema_registry_url=\"http://localhost:8081\"\n",
    ")\n",
    "\n",
    "# Create AGStream for answers\n",
    "answer_stream = AGStream(\n",
    "    atype=Answer,\n",
    "    input_topic=\"tutorial-answers\",\n",
    "    output_topic=\"tutorial-questions\",\n",
    "    schema_registry_url=\"http://localhost:8081\"\n",
    ")\n",
    "\n",
    "print(f\"Question stream configured for topic: {question_stream.input_topic}\")\n",
    "print(f\"Answer stream configured for topic: {answer_stream.input_topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Register Schemas in Schema Registry\n",
    "\n",
    "Let's register our Pydantic models as JSON Schemas in the schema registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register Question schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_schema_id = register_atype_schema(atype=Question, schema_registry_url=\"http://localhost:8081\")\n",
    "print(f\"âœ… Question schema registered with ID: {question_schema_id}\")\n",
    "\n",
    "# Register Answer schema\n",
    "answer_schema_id = register_atype_schema(atype=Answer, schema_registry_url=\"http://localhost:8081\")\n",
    "print(f\"âœ… Answer schema registered with ID: {answer_schema_id}\")\n",
    "\n",
    "# List registered schemas\n",
    "question_versions = list_schema_versions(atype_name=\"Question\", schema_registry_url=\"http://localhost:8081\")\n",
    "answer_versions = list_schema_versions(atype_name=\"Answer\", schema_registry_url=\"http://localhost:8081\")\n",
    "\n",
    "print(f\"\\nQuestion schema versions: {question_versions}\")\n",
    "print(f\"Answer schema versions: {answer_versions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Kafka Topics\n",
    "\n",
    "Before we can produce and consume messages, we need to create the Kafka topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create topics if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in [\"tutorial-questions\", \"tutorial-answers\"]:\n",
    "    if not kafka_topic_exists(topic):\n",
    "        create_kafka_topic(topic)\n",
    "        print(f\"âœ… Created topic: {topic}\")\n",
    "        time.sleep(1)  # Give Kafka time to create the topic\n",
    "    else:\n",
    "        print(f\"âœ… Topic already exists: {topic}\")\n",
    "\n",
    "print(\"\\nAll topics are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Listener in Background\n",
    "\n",
    "Let's set up a listener that will process questions and generate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a listener stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener = AGStream(\n",
    "    atype=Answer,\n",
    "    input_topic=\"tutorial-questions\",\n",
    "    output_topic=\"tutorial-answers\",\n",
    "    schema_registry_url=\"http://localhost:8081\"\n",
    ")\n",
    "\n",
    "# Start the listener in a separate thread\n",
    "def listener_thread():\n",
    "    print(\"\\nğŸ§ Starting listener...\")\n",
    "    try:\n",
    "        # Listen for questions and produce answers\n",
    "        listener.listen(source_atype_name=\"Question\")\n",
    "    except Exception as e:\n",
    "        print(f\"Listener error: {e}\")\n",
    "        message_queue.put(f\"Listener error: {e}\")\n",
    "\n",
    "# Start listener in background\n",
    "listener_thread = threading.Thread(target=listener_thread, daemon=True)\n",
    "listener_thread.start()\n",
    "\n",
    "print(\"Listener started in background. It will process questions and generate answers.\")\n",
    "print(\"Waiting 5 seconds for listener to initialize...\")\n",
    "time.sleep(5)  # Give listener time to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Produce and Consume Messages\n",
    "\n",
    "Now let's create a complete workflow: produce questions, let the listener process them, and then consume the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    Question(text=\"What is machine learning?\", category=\"AI\", priority=3),\n",
    "    Question(text=\"How does blockchain work?\", category=\"Technology\", priority=2),\n",
    "    Question(text=\"What is the capital of France?\", category=\"Geography\", priority=2)\n",
    "]\n",
    "\n",
    "# Function to send questions and collect answers\n",
    "def send_and_collect():\n",
    "    # Send questions to Kafka\n",
    "    question_keys = []\n",
    "    for question in questions:\n",
    "        question_stream.states = [question]\n",
    "        msg_ids = question_stream.produce(register_if_missing=True)\n",
    "        key = msg_ids[0] if msg_ids else None\n",
    "        question_keys.append(key)\n",
    "        print(f\"âœ… Sent question: {question.text[:30]}... (key: {key})\")\n",
    "        time.sleep(1)  # Small delay between messages\n",
    "    \n",
    "    # Wait for processing\n",
    "    print(\"\\nğŸ•’ Waiting for answers to be processed...\")\n",
    "    time.sleep(5)  # Give listener time to process\n",
    "    \n",
    "    # Create answer collector\n",
    "    answer_collector = AGStream(\n",
    "        input_topic=\"tutorial-answers\",\n",
    "        schema_registry_url=\"http://localhost:8081\"\n",
    "    )\n",
    "    \n",
    "    # Retrieve Answer type from registry\n",
    "    RetrievedAnswer = get_atype_from_registry(atype_name=\"Answer\", schema_registry_url=\"http://localhost:8081\")\n",
    "    if RetrievedAnswer:\n",
    "        answer_collector.atype = RetrievedAnswer\n",
    "        print(f\"âœ… Retrieved Answer type from registry: {RetrievedAnswer.__name__}\")\n",
    "    \n",
    "    # Collect answers\n",
    "    answers = answer_collector.collect_sources(\n",
    "        max_messages=5,\n",
    "        timeout_ms=10000,  # Longer timeout\n",
    "        mode='latest',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Collected {len(answers)} answers:\")\n",
    "    for i, answer in enumerate(answers, 1):\n",
    "        if answer.states:\n",
    "            print(f\"{i}. {answer.states[0].text[:50]}... (Confidence: {answer.states[0].confidence})\")\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# Run the workflow\n",
    "answers = send_and_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Schema Evolution Example\n",
    "\n",
    "Let's demonstrate how schema evolution works with the registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an updated version of our Question model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionV2(BaseModel):\n",
    "    \"\"\"Updated question model with additional fields\"\"\"\n",
    "    text: str = Field(description=\"The question text\")\n",
    "    category: str = Field(description=\"Question category\")\n",
    "    priority: int = Field(default=1, description=\"Priority level (1-5)\")\n",
    "    language: str = Field(default=\"en\", description=\"Language of the question\")\n",
    "    timestamp: float = Field(default_factory=time.time, description=\"When the question was asked\")\n",
    "\n",
    "# Create a new stream with the updated type\n",
    "updated_stream = AGStream(\n",
    "    atype=QuestionV2,\n",
    "    input_topic=\"tutorial-questions\",\n",
    "    schema_registry_url=\"http://localhost:8081\"\n",
    ")\n",
    "\n",
    "# Register the updated schema\n",
    "updated_schema_id = register_atype_schema(atype=QuestionV2, schema_registry_url=\"http://localhost:8081\")\n",
    "print(f\"âœ… Updated Question schema registered with ID: {updated_schema_id}\")\n",
    "\n",
    "# List all versions\n",
    "all_versions = list_schema_versions(atype_name=\"QuestionV2\", schema_registry_url=\"http://localhost:8081\")\n",
    "print(f\"All Question schema versions: {all_versions}\")\n",
    "\n",
    "# Retrieve version 1 (original)\n",
    "OriginalQuestion = get_atype_from_registry(atype_name=\"QuestionV2\", schema_registry_url=\"http://localhost:8081\", version=\"1\")\n",
    "print(f\"Original Question type: {OriginalQuestion.__name__ if OriginalQuestion else 'Not found'}\")\n",
    "\n",
    "# Retrieve latest version\n",
    "LatestQuestion = get_atype_from_registry(atype_name=\"QuestionV2\", schema_registry_url=\"http://localhost:8081\", version=\"latest\")\n",
    "print(f\"Latest Question type: {LatestQuestion.__name__ if LatestQuestion else 'Not found'}\")\n",
    "\n",
    "# Show the difference\n",
    "if OriginalQuestion and LatestQuestion:\n",
    "    print(\"\\nğŸ”„ Schema Evolution:\")\n",
    "    orig_fields = set(OriginalQuestion.model_fields.keys())\n",
    "    latest_fields = set(LatestQuestion.model_fields.keys())\n",
    "    print(f\"   Original fields: {orig_fields}\")\n",
    "    print(f\"   Latest fields: {latest_fields}\")\n",
    "    print(f\"   Added fields: {latest_fields - orig_fields}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Direct Schema Registry Access\n",
    "\n",
    "Let's explore the schema registry directly using the REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# List all subjects\n",
    "def list_subjects():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8081/subjects\")\n",
    "        if response.status_code == 200:\n",
    "            subjects = response.json()\n",
    "            print(\"ğŸ“‹ Registered Subjects:\")\n",
    "            for subject in subjects:\n",
    "                print(f\"   - {subject}\")\n",
    "            return subjects\n",
    "        else:\n",
    "            print(f\"âŒ Error listing subjects: {response.status_code} - {response.text}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get schema details\n",
    "def get_schema_details(subject: str, version: str = \"latest\"):\n",
    "    try:\n",
    "        url = f\"http://localhost:8081/subjects/{subject}/versions/{version}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            details = response.json()\n",
    "            print(f\"\\nğŸ“„ Schema Details for {subject} (version {version}):\")\n",
    "            print(f\"   ID: {details.get('id')}\")\n",
    "            print(f\"   Version: {details.get('version')}\")\n",
    "            print(f\"   Schema: {details.get('schema')[:100]}...\")\n",
    "            return details\n",
    "        else:\n",
    "            print(f\"âŒ Error getting schema: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# List all subjects\n",
    "subjects = list_subjects()\n",
    "\n",
    "# Get details for each subject\n",
    "if subjects:\n",
    "    for subject in subjects:\n",
    "        get_schema_details(subject)\n",
    "        time.sleep(0.5)  # Small delay between requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Schema Validation\n",
    "\n",
    "Let's demonstrate how schema validation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentics.core.streaming_utils import get_schema_info\n",
    "\n",
    "# Validate a message dict against a registered schema using streaming_utils.\n",
    "# The subject name is derived from the Pydantic class name (e.g. 'Question-value'),\n",
    "# NOT from the Kafka topic name.\n",
    "def validate_message(message: dict, atype_name: str,\n",
    "                     schema_registry_url: str = \"http://localhost:8081\") -> bool:\n",
    "    \"\"\"Validate a message dict against a schema fetched from the registry.\"\"\"\n",
    "    info = get_schema_info(\n",
    "        atype_name=atype_name,\n",
    "        schema_registry_url=schema_registry_url,\n",
    "        is_key=False,\n",
    "        version=\"latest\",\n",
    "        add_suffix=True,\n",
    "    )\n",
    "    if info is None:\n",
    "        print(f\"âŒ Schema not found for '{atype_name}-value'\")\n",
    "        return False\n",
    "\n",
    "    schema = info[\"schema\"]\n",
    "    required_fields = schema.get(\"required\", [])\n",
    "\n",
    "    for field in required_fields:\n",
    "        if field not in message:\n",
    "            print(f\"âŒ Missing required field: {field}\")\n",
    "            return False\n",
    "\n",
    "    print(f\"âœ… Message is valid against schema '{info['subject']}' (ID: {info['id']})\")\n",
    "    return True\n",
    "\n",
    "# Test validation with a valid message\n",
    "# Note: subject is derived from the class name 'Question', not the topic name.\n",
    "test_message = {\n",
    "    \"text\": \"Test question\",\n",
    "    \"category\": \"Test\",\n",
    "    \"priority\": 1\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ” Testing message validation:\")\n",
    "is_valid = validate_message(test_message, \"Question\")\n",
    "print(f\"Validation result: {'âœ… PASS' if is_valid else 'âŒ FAIL'}\")\n",
    "\n",
    "# Test with invalid message (missing required field)\n",
    "invalid_message = {\n",
    "    \"text\": \"Invalid question\",\n",
    "    \"priority\": 1\n",
    "    # Missing 'category' which is required\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ” Testing invalid message:\")\n",
    "is_valid = validate_message(invalid_message, \"Question\")\n",
    "print(f\"Validation result: {'âœ… PASS' if is_valid else 'âŒ FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup and Summary\n",
    "\n",
    "Let's clean up our resources and summarize what we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ§¹ Cleanup complete!\")\n",
    "print(\"\\nğŸ“š Summary of what we've learned:\")\n",
    "print(\"\\n1. âœ… Schema Registration: Stored Pydantic models as JSON Schemas\")\n",
    "print(\"2. âœ… Schema Retrieval: Dynamically loaded types from registry\")\n",
    "print(\"3. âœ… Message Production: Sent type-safe messages to Kafka\")\n",
    "print(\"4. âœ… Message Consumption: Retrieved messages using registered schemas\")\n",
    "print(\"5. âœ… Listener Setup: Processed messages with schema validation\")\n",
    "print(\"6. âœ… Schema Evolution: Managed versioning and compatibility\")\n",
    "print(\"7. âœ… Direct API Access: Explored schema registry REST API\")\n",
    "print(\"8. âœ… Schema Validation: Validated messages against schemas\")\n",
    "\n",
    "print(\"\\nğŸ¯ Key Benefits:\")\n",
    "print(\"- Type Safety: Ensure messages conform to registered schemas\")\n",
    "print(\"- Schema Evolution: Track changes and maintain compatibility\")\n",
    "print(\"- Dynamic Discovery: Load types at runtime without hardcoding\")\n",
    "print(\"- Centralized Management: Store all schemas in one place\")\n",
    "\n",
    "print(\"\\nğŸš€ Next Steps:\")\n",
    "print(\"- Explore Schema Registry UI: http://localhost:8000\")\n",
    "print(\"- Check Kafka UI: http://localhost:8080\")\n",
    "print(\"- Try creating your own models and integrating them\")\n",
    "print(\"- Experiment with more complex schema evolution scenarios\")\n",
    "print(\"- Integrate with your production streaming pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. `listen` â€” LLM Transduction on the Stream\n",
    "\n",
    "The `listen` method combines schema-validated Kafka consumption\n",
    "with **logical transduction**: for every incoming state it calls `self << source` (the\n",
    "AGStream `__lshift__` operator) which invokes the configured LLM to convert the source\n",
    "object into the target type, then produces the result back to the output topic.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "```\n",
    "input_topic  â”€â”€â–º  validate (schema registry)  â”€â”€â–º  transduce (LLM)  â”€â”€â–º  output_topic\n",
    "  Question                                           Question â†’ MovieSummary\n",
    "```\n",
    "\n",
    "### Key parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|---|---|---|\n",
    "| `source_atype_name` | `None` | Fetch source type from registry by name |\n",
    "| `validate_schema` | `True` | Reject states that fail Pydantic validation |\n",
    "| `produce_results` | `True` | Write transduced states to `output_topic` |\n",
    "| `verbose` | `False` | Print per-state progress |\n",
    "| `group_id` | auto UUID | Kafka consumer group (unique â†’ always reads from earliest) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define source and target Pydantic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from agentics.core.streaming import AGStream\n",
    "import time\n",
    "import threading\n",
    "\n",
    "class MovieReview(BaseModel):\n",
    "    \"\"\"A raw movie review as it arrives on the stream.\"\"\"\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    review: str = Field(description=\"Full review text\")\n",
    "    rating: float = Field(description=\"Numeric rating 1-10\")\n",
    "\n",
    "class MovieSummary(BaseModel):\n",
    "    \"\"\"A concise summary produced by the LLM transducer.\"\"\"\n",
    "    title: str = Field(description=\"Movie title\")\n",
    "    one_line_summary: str = Field(description=\"One-sentence summary of the review\")\n",
    "    sentiment: str = Field(description=\"Overall sentiment: positive / neutral / negative\")\n",
    "\n",
    "print(\"MovieReview fields :\", list(MovieReview.model_fields.keys()))\n",
    "print(\"MovieSummary fields:\", list(MovieSummary.model_fields.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Kafka topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in [\"movie-reviews\", \"movie-summaries\"]:\n",
    "    if not kafka_topic_exists(topic):\n",
    "        create_kafka_topic(topic)\n",
    "        print(f\"âœ… Created topic: {topic}\")\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(f\"âœ… Topic already exists: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Register schemas in the schema registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_stream = AGStream(\n",
    "    atype=MovieReview,\n",
    "    input_topic=\"movie-reviews\",\n",
    "    output_topic=\"movie-summaries\",\n",
    "    schema_registry_url=\"http://localhost:8081\",\n",
    ")\n",
    "\n",
    "summary_stream = AGStream(\n",
    "    atype=MovieSummary,\n",
    "    input_topic=\"movie-summaries\",\n",
    "    output_topic=\"movie-summaries\",\n",
    "    schema_registry_url=\"http://localhost:8081\",\n",
    ")\n",
    "\n",
    "review_schema_id  = register_atype_schema(atype=MovieReview, schema_registry_url=\"http://localhost:8081\")\n",
    "summary_schema_id = register_atype_schema(atype=MovieSummary, schema_registry_url=\"http://localhost:8081\")\n",
    "\n",
    "print(f\"âœ… MovieReview  schema registered (ID: {review_schema_id})\")\n",
    "print(f\"âœ… MovieSummary schema registered (ID: {summary_schema_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the transducing listener\n",
    "\n",
    "The listener AGStream is configured with:\n",
    "  â€¢ atype          = MovieSummary  (the TARGET type the LLM must produce)\n",
    "  â€¢ input_topic    = \"movie-reviews\"  (where raw reviews arrive)\n",
    "  â€¢ output_topic   = \"movie-summaries\"  (where summaries are written)\n",
    "  â€¢ instructions   = task description passed to the LLM\n",
    "\n",
    "listen() will:\n",
    "  1. Poll \"movie-reviews\" continuously\n",
    "  2. Validate each incoming state as a MovieReview\n",
    "  3. Call  listener << source_ag  (LLM transduction)\n",
    "  4. Produce the resulting MovieSummary to \"movie-summaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener = AGStream(\n",
    "    atype=MovieSummary,\n",
    "    input_topic=\"movie-reviews\",\n",
    "    output_topic=\"movie-summaries\",\n",
    "    schema_registry_url=\"http://localhost:8081\",\n",
    "    instructions=(\n",
    "        \"You receive a movie review. \"\n",
    "        \"Produce a MovieSummary with a one-sentence summary and the overall sentiment.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Listener configured:\")\n",
    "print(f\"  input_topic  : {listener.input_topic}\")\n",
    "print(f\"  output_topic : {listener.output_topic}\")\n",
    "print(f\"  target type  : {listener.atype.__name__}\")\n",
    "print(f\"  instructions : {listener.instructions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Ensure schemas are registered, then start the listener\n",
    "\n",
    "IMPORTANT: The listener fetches the source schema ('MovieReview-value') from\n",
    "the schema registry on startup.  If Step 3 was skipped or the registry was\n",
    "not yet ready, the fetch would fail with a 404.  We guard against this by\n",
    "(a) registering both schemas here if they are missing, and\n",
    "(b) passing schema_fetch_retries so the listener retries a few times with\n",
    "    a short delay before giving up.\n",
    "\n",
    "Guard: register schemas if they haven't been registered yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for atype, name in [(MovieReview, 'MovieReview'), (MovieSummary, 'MovieSummary')]:\n",
    "    versions = list_schema_versions(atype_name=name, schema_registry_url=\"http://localhost:8081\")\n",
    "    if not versions:\n",
    "        schema_id = register_atype_schema(atype=atype, schema_registry_url=\"http://localhost:8081\")\n",
    "        print(f\"âœ… Registered {name} schema (ID: {schema_id})\")\n",
    "    else:\n",
    "        print(f\"âœ… {name} schema already registered (versions: {versions})\")\n",
    "\n",
    "def run_listener():\n",
    "    try:\n",
    "        listener.listen(\n",
    "            source_atype_name=\"MovieReview\",  # fetch source type from registry\n",
    "            validate_schema=True,\n",
    "            produce_results=True,\n",
    "            verbose=True,\n",
    "            schema_fetch_retries=5,        # retry up to 5 times if schema not found\n",
    "            schema_fetch_retry_delay=2.0,  # wait 2 s between retries\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(f\"Listener thread exited with error: {exc}\")\n",
    "\n",
    "listener_thread = threading.Thread(target=run_listener, daemon=True)\n",
    "listener_thread.start()\n",
    "\n",
    "print(\"ğŸ§ Listener thread started. Waiting 3 s for it to initialise...\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Produce some MovieReview messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    MovieReview(\n",
    "        title=\"Inception\",\n",
    "        review=(\n",
    "            \"A mind-bending thriller that challenges the boundaries of reality. \"\n",
    "            \"Nolan's direction is masterful and the performances are outstanding.\"\n",
    "        ),\n",
    "        rating=9.2,\n",
    "    ),\n",
    "    MovieReview(\n",
    "        title=\"The Room\",\n",
    "        review=(\n",
    "            \"Widely considered one of the worst films ever made. \"\n",
    "            \"The dialogue is stilted, the plot makes no sense, and the acting is painful.\"\n",
    "        ),\n",
    "        rating=1.8,\n",
    "    ),\n",
    "    MovieReview(\n",
    "        title=\"Parasite\",\n",
    "        review=(\n",
    "            \"A brilliant social commentary wrapped in a gripping thriller. \"\n",
    "            \"Bong Joon-ho delivers a near-perfect film that deserved every Oscar it won.\"\n",
    "        ),\n",
    "        rating=9.5,\n",
    "    ),\n",
    "]\n",
    "\n",
    "producer_stream = AGStream(\n",
    "    atype=MovieReview,\n",
    "    input_topic=\"movie-reviews\",\n",
    "    schema_registry_url=\"http://localhost:8081\",\n",
    ")\n",
    "producer_stream.states = sample_reviews\n",
    "\n",
    "msg_ids = producer_stream.produce(register_if_missing=True)\n",
    "print(f\"\\nâœ… Produced {len(msg_ids)} MovieReview messages to 'movie-reviews'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Wait for transduction, then collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â³ Waiting 15 s for the listener to transduce all reviews...\")\n",
    "time.sleep(15)\n",
    "\n",
    "# Collect the MovieSummary objects from the output topic\n",
    "collector = AGStream(\n",
    "    atype=MovieSummary,\n",
    "    input_topic=\"movie-summaries\",\n",
    "    schema_registry_url=\"http://localhost:8081\",\n",
    ")\n",
    "\n",
    "results = collector.collect_sources(\n",
    "    mode=\"all\",\n",
    "    max_messages=20,\n",
    "    timeout_ms=8000,\n",
    "    validate_schema=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Collected {len(results)} MovieSummary objects from 'movie-summaries':\")\n",
    "for i, ag in enumerate(results, 1):\n",
    "    if ag.states:\n",
    "        s = ag.states[0]\n",
    "        print(f\"\\n  [{i}] {s.title}\")\n",
    "        print(f\"       Summary  : {s.one_line_summary}\")\n",
    "        print(f\"       Sentiment: {s.sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Demonstrate produce_results=False (dry-run: transduce but don't write)\n",
    "\n",
    "Sometimes you want to transduce a single message and inspect the result\n",
    "without writing it back to Kafka.  Set produce_results=False and the\n",
    "listener will still transduce but skip the produce step.\n",
    "\n",
    "Key settings for a clean dry-run:\n",
    "  â€¢ auto_offset_reset='latest'  â€” only process messages produced AFTER the\n",
    "                                  listener starts; avoids replaying old messages\n",
    "  â€¢ max_empty_polls=5           â€” exit automatically once the topic goes quiet\n",
    "  â€¢ produce_results=False       â€” transduce locally, nothing written to Kafka\n",
    "\n",
    "8a: Produce one fresh review BEFORE starting the listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_review = MovieReview(\n",
    "    title=\"Everything Everywhere All at Once\",\n",
    "    review=(\n",
    "        \"An absurdist multiverse adventure that somehow manages to be deeply emotional. \"\n",
    "        \"A genre-defying masterpiece.\"\n",
    "    ),\n",
    "    rating=9.0,\n",
    ")\n",
    "dry_run_producer = AGStream(\n",
    "    atype=MovieReview,\n",
    "    input_topic=\"movie-reviews\",\n",
    "    schema_registry_url=\"http://localhost:8081\",\n",
    ")\n",
    "dry_run_producer.states = [extra_review]\n",
    "dry_run_producer.produce(register_if_missing=True)\n",
    "print(\"âœ… Extra review produced to 'movie-reviews'.\")\n",
    "\n",
    "# â”€â”€ 8b: Run the dry-run listener â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dry_run_listener = AGStream(\n",
    "    atype=MovieSummary,\n",
    "    input_topic=\"movie-reviews\",\n",
    "    output_topic=\"movie-summaries\",\n",
    "    schema_registry_url=\"http://localhost:8081\",\n",
    "    instructions=(\n",
    "        \"You receive a movie review. \"\n",
    "        \"Produce a MovieSummary with a one-sentence summary and the overall sentiment.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Running dry-run listener (produce_results=False, auto_offset_reset='latest')...\")\n",
    "\n",
    "dry_run_listener.listen(\n",
    "    source_atype_name=\"MovieReview\",\n",
    "    validate_schema=True,\n",
    "    produce_results=False,         # <â”€â”€ transduce locally, do NOT write to Kafka\n",
    "    verbose=True,\n",
    "    auto_offset_reset=\"latest\",    # <â”€â”€ only process messages produced after start\n",
    "    max_empty_polls=5,             # exit after 5 consecutive empty polls\n",
    "    schema_fetch_retries=5,\n",
    "    schema_fetch_retry_delay=2.0,\n",
    ")\n",
    "\n",
    "print(\"\\nDry-run complete â€” no messages were written to 'movie-summaries'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary â€” `listen`\n",
    "\n",
    "| Step | What happens |\n",
    "|---|---|\n",
    "| **Poll** | Continuously polls `input_topic` using a Kafka consumer |\n",
    "| **Validate** | Each state dict is instantiated as `source_atype` (Pydantic validation) |\n",
    "| **Transduce** | `asyncio.run(transducer.__lshift__(source_ag))` â€” LLM converts source â†’ target |\n",
    "| **Produce** | Transduced states are sent to `output_topic` via `produce` |\n",
    "| **Stop** | `KeyboardInterrupt` (or Stop button) closes the consumer gracefully |\n",
    "\n",
    "The method is the **streaming counterpart** of the batch `<<` operator: instead of\n",
    "transducing a whole list at once, it processes one state at a time as it arrives on\n",
    "the Kafka topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. `aproduce_and_collect` â€” Async Fire-and-Collect\n",
    "\n",
    "`aproduce_and_collect` is the **async counterpart** to the `listen` + `collect_sources`\n",
    "pattern.  Instead of managing a background listener thread and a separate collector,\n",
    "it does everything in one `await`:\n",
    "\n",
    "1. **Produces** every state in `self.states` to `input_topic`, each tagged with a\n",
    "   unique UUID key.\n",
    "2. **Starts a background consumer** that polls `output_topic` and matches incoming\n",
    "   messages by their Kafka key.\n",
    "3. **Awaits** until every key has been collected or `timeout` seconds have elapsed.\n",
    "4. **Returns** the results in the **same order** as the original `self.states` list.\n",
    "\n",
    "### When to use it\n",
    "\n",
    "| Scenario | Recommended method |\n",
    "|---|---|\n",
    "| Long-running daemon that processes an unbounded stream | `listen()` |\n",
    "| One-shot batch: produce N items, collect N results, done | `aproduce_and_collect()` |\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "A `listen()` worker **must already be running** on the same `input_topic â†’ output_topic`\n",
    "pipeline.  `aproduce_and_collect` only produces and collects â€” it does **not** transduce.\n",
    "\n",
    "### Key parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|---|---|---|\n",
    "| `result_atype` | `None` | Pydantic model class for the **output** (target) type â€” use when the output topic carries a different schema than the input topic |\n",
    "| `timeout` | `120.0` | Max seconds to wait for all results |\n",
    "| `poll_interval` | `0.5` | Seconds between output-topic poll cycles |\n",
    "| `register_if_missing` | `True` | Auto-register source schema before producing |\n",
    "| `verbose` | `False` | Print per-message progress to stderr |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Make sure the listener from Section 12 is still running\n",
    "\n",
    "The listener_thread started in Section 12 (run_listener / listener.listen)\n",
    "is a daemon thread, so it keeps running as long as the kernel is alive.\n",
    "If you restarted the kernel, re-run the listener cell in Section 12 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "\n",
    "print(f\"Listener thread alive: {listener_thread.is_alive()}\")\n",
    "if not listener_thread.is_alive():\n",
    "    print(\"âš ï¸  Listener is not running â€” re-starting it now...\")\n",
    "    listener_thread = threading.Thread(target=run_listener, daemon=True)\n",
    "    listener_thread.start()\n",
    "    import time; time.sleep(3)\n",
    "    print(\"ğŸ§ Listener restarted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the producer AGStream\n",
    "\n",
    "The producer is configured with:\n",
    "  â€¢ atype         = MovieReview   (the type we are producing)\n",
    "  â€¢ input_topic   = 'movie-reviews'   (where we send the raw reviews)\n",
    "  â€¢ output_topic  = 'movie-summaries' (where we expect the transduced results)\n",
    "\n",
    "aproduce_and_collect() will:\n",
    "  1. Produce all states to 'movie-reviews' with unique UUID keys\n",
    "  2. Poll 'movie-summaries' for messages whose key matches one of those UUIDs\n",
    "  3. Return the results in the same order as sample_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_producer = AGStream(\n",
    "    atype=MovieReview,\n",
    "    input_topic=\"movie-reviews\",\n",
    "    output_topic=\"movie-summaries\",\n",
    "    schema_registry_url=\"http://localhost:8081\",\n",
    ")\n",
    "\n",
    "pac_reviews = [\n",
    "    MovieReview(\n",
    "        title=\"Oppenheimer\",\n",
    "        review=(\n",
    "            \"A three-hour epic that never drags. Nolan's most mature work, \"\n",
    "            \"blending historical drama with existential dread.\"\n",
    "        ),\n",
    "        rating=9.0,\n",
    "    ),\n",
    "    MovieReview(\n",
    "        title=\"Barbie\",\n",
    "        review=(\n",
    "            \"Surprisingly subversive and visually inventive. \"\n",
    "            \"Gerwig turns a toy commercial into a sharp feminist comedy.\"\n",
    "        ),\n",
    "        rating=8.1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "pac_producer.states = pac_reviews\n",
    "\n",
    "print(f\"Prepared {len(pac_reviews)} reviews for aproduce_and_collect:\")\n",
    "for r in pac_reviews:\n",
    "    print(f\"  â€¢ {r.title} (rating: {r.rating})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Call aproduce_and_collect\n",
    "\n",
    "In a Jupyter notebook the event loop is already running (managed by IPython /\n",
    "nest_asyncio), so we use `await` directly.  In a plain Python script you would\n",
    "use `asyncio.run(pac_producer.aproduce_and_collect(...))` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await pac_producer.aproduce_and_collect(\n",
    "    result_atype=MovieSummary,\n",
    "    register_if_missing=True,\n",
    "    timeout=90.0,\n",
    "    poll_interval=1.0,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“‹ aproduce_and_collect returned {len(results)} result(s):\")\n",
    "for review, result_ag in zip(pac_reviews, results):\n",
    "    if result_ag and result_ag.states:\n",
    "        s = result_ag.states[0]\n",
    "        print(f\"\\n  [{review.title}]\")\n",
    "        print(f\"    Summary  : {s.one_line_summary}\")\n",
    "        print(f\"    Sentiment: {s.sentiment}\")\n",
    "    else:\n",
    "        print(f\"\\n  [{review.title}] âš ï¸  No result received (timeout or error)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary â€” `aproduce_and_collect`\n",
    "\n",
    "| Step | What happens |\n",
    "|---|---|\n",
    "| **Probe** | Records the current end-offset of `output_topic` to ignore stale messages |\n",
    "| **Produce** | Sends all states to `input_topic` with unique UUID keys |\n",
    "| **Consume** | Background thread polls `output_topic`, matches messages by key |\n",
    "| **Await** | Async loop checks every `poll_interval` seconds until all keys collected or timeout |\n",
    "| **Return** | Results list in the **same order** as the input states (`None` for timeouts) |\n",
    "\n",
    "**Comparison with `listen` + `collect_sources`:**\n",
    "\n",
    "```python\n",
    "# â”€â”€ Old pattern (manual orchestration)\n",
    "producer.states = reviews\n",
    "producer.produce(register_if_missing=True)\n",
    "time.sleep(15)  # hope the listener is fast enough\n",
    "results = collector.collect_sources(mode='all', max_messages=20)\n",
    "\n",
    "# â”€â”€ New pattern (aproduce_and_collect)\n",
    "producer.states = reviews\n",
    "results = await producer.aproduce_and_collect(timeout=90)\n",
    "# results[i] corresponds exactly to reviews[i] â€” no guessing, no sleep()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
