{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udf10 Agentics","text":"<p>Agentics is a lightweight, Python-native framework for building structured and massively parallel agentic workflows using Pydantic models and transducible functions . </p>"},{"location":"#documentation-overview","title":"\ud83d\udcda Documentation Overview","text":"<ul> <li> <p>Getting Started \ud83d\ude80   Install Agentics, set up your environment, and run your first transducible function over a small dataset.</p> </li> <li> <p>Core Concepts \ud83e\udde0   The mental model: Pydantic types, transducible functions, typed state containers, Logical Transduction Algebra (LTA), and Map\u2013Reduce.</p> </li> <li> <p>Transducible Functions \u2699\ufe0f   How to define, configure, and invoke transducible functions; specifying instructions; controlling temperature, retries, and structured decoding.</p> </li> <li> <p>Agentics \ud83e\uddec   Defining Pydantic models for inputs/outputs, working with <code>AG</code> containers, loading data from JSON/CSV/DataFrames, and preserving type information across the pipeline.</p> </li> <li> <p>Logical Transduction Algebra \ud83d\udd01   Chaining transducible functions, branching, fan-in/fan-out patterns, and building reusable pipeline components.</p> </li> <li> <p>Async Map\u2013Reduce Execution \ud83d\ude80   Using <code>amap</code> and <code>areduce</code> for large-scale runs, batching strategies, handling failures, and performance considerations.</p> </li> <li> <p>Examples &amp; Use Cases \ud83d\udcd8   End-to-end examples: text-to-SQL, data extraction and enrichment, classification, document workflows, evaluation pipelines, and more.</p> </li> </ul>"},{"location":"#transducible-functions","title":"Transducible Functions","text":"<p>A transducible function is an LLM-powered, type-safe transformation between Pydantic models. Agentics lets you:</p> <ul> <li>Define these transformations declaratively </li> <li>Compose them into pipelines </li> <li>Execute them at scale using an asynchronous Map\u2013Reduce execution engine \u2699\ufe0f</li> </ul> <p>Under the hood, Agentics is grounded in Logical Transduction Algebra (LTA), a logico-mathematical formalism that guarantees:</p> <ul> <li>\u2705 Composability  </li> <li>\u2705 Explainability  </li> <li>\u2705 Stability of LLM-based transformations  </li> </ul> <p>The result is a way to build agentic systems that are:</p> <ul> <li>Typed \u2013 every step has explicit input/output schemas \ud83d\udcd0  </li> <li>Composable \u2013 pipelines are built from reusable transducible functions \ud83e\udde9  </li> <li>Traceable \u2013 outputs carry evidence back to input fields \ud83d\udd0d  </li> <li>Scalable \u2013 async <code>amap</code> / <code>areduce</code> primitives support large workloads \ud83d\ude80  </li> <li>Minimal \u2013 no heavy orchestrators: just types, functions, and data \ud83e\udeb6  </li> </ul> <p>Agentics code is simple, predictable, and robust, and is easy to embed into modern ecosystems (LangFlow, LangChain, CrewAI, MCP, etc.) \ud83e\udd1d.</p>"},{"location":"#key-features","title":"\ud83d\udd11 Key Features","text":""},{"location":"#transducible-functions-core-abstraction","title":"\u2699\ufe0f Transducible Functions (Core Abstraction)","text":"<p>Define LLM-powered transformations as first-class functions:</p> <ul> <li>\ud83e\uddfe Typed input and output via Pydantic models  </li> <li>\ud83d\udee1\ufe0f Automatic schema validation and type-constrained generation  </li> <li>\ud83e\ude9c Composable into higher-level workflows and chains  </li> </ul>"},{"location":"#typed-state-containers-aka-agentics-ag","title":"\ud83e\uddf1 Typed State Containers - a.k.a. Agentics (AG)","text":"<p>Wrap data into typed state collections so that every row or document carries a concrete Pydantic type:</p> <ul> <li>Safe, batch-level operations \u2705  </li> <li>Clear semantics over datasets and intermediate states \ud83d\udcca  </li> <li>Input/output from DBs, CSV and Json</li> <li>Ideal to represent tabular/structured data</li> </ul>"},{"location":"#async-mapreduce-execution","title":"\ud83d\ude80 Async Map\u2013Reduce Execution","text":"<p>Run transducible functions over large collections using:</p> <ul> <li>\u26a1 <code>amap</code> for massively parallel application  </li> <li>\ud83d\udcc9 <code>areduce</code> for aggregations and global summaries  </li> </ul> <p>Designed to scale on multi-core or distributed execution backends \ud83d\udda5\ufe0f\ud83d\udda5\ufe0f\ud83d\udda5\ufe0f.</p>"},{"location":"#dynamic-type-function-composition","title":"\ud83e\udde9 Dynamic Type &amp; Function Composition","text":"<p>Create new workflows on the fly:</p> <ul> <li>\ud83d\udd04 Merge or refine types dynamically  </li> <li>\ud83e\uddec Compose transducible functions declaratively  </li> <li>\ud83d\udd00 Build polymorphic or adaptive pipelines driven by data and instructions  </li> </ul>"},{"location":"#explainable-traceable-inference","title":"\ud83d\udd0d Explainable &amp; Traceable Inference","text":"<p>Each generated attribute can be traced back to:</p> <ul> <li>Specific input fields \ud83e\uddf7  </li> <li>The specific transducible function or step that produced it \ud83e\udde0  </li> </ul> <p>This enables auditable, debuggable LLM reasoning across the pipeline.</p>"},{"location":"#end-to-end-type-safety","title":"\ud83d\udee1\ufe0f End-to-End Type Safety","text":"<p>Pydantic models are enforced at every boundary:</p> <ul> <li>\u2705 Validation on input loading  </li> <li>\u2705 Validation after each transducible function  </li> <li>\u2705 Predictable runtime behavior and clear failure modes  </li> </ul>"},{"location":"#tool-integration","title":"\ud83d\udd0c Tool Integration","text":"<p>Agentics is fully compatible with Model Context Protocol (MCP) and expose external tools and knowledge to transducible functions:</p> <ul> <li>\ud83c\udf10 Web / search tools  </li> <li>\ud83d\uddc4\ufe0f Databases &amp; vector stores  </li> <li>\ud83d\udcbb Code execution backends  </li> <li>\ud83d\udd17 MCP-based tools  </li> </ul>"},{"location":"#minimalistic-pythonic-api","title":"\u2728 Minimalistic, Pythonic API","text":"<p>The framework is intentionally small:</p> <ul> <li>\ud83d\udeab No custom DSL to learn  </li> <li>\ud83d\udc0d Just Python functions, Pydantic models, and a few core primitives  </li> <li>\ud83c\udf09 Easy to embed into existing stacks (LangFlow nodes, CrewAI agents, MCPs, etc.)  </li> </ul>"},{"location":"agentics/","title":"Agentics","text":"<p>Agentics objects are wrappers around list of objects having the same Pydantic Type. They are designed to enable async logical transduction among their instances. Agentics enable us to think about AI workflows in terms of structured data transformations rather than agent behaviours, knowledge and tasks. </p>"},{"location":"agentics/#the-agentics-class","title":"The Agentics class","text":"<p>Agentics is a Python class that wraps a list of Pydantic objects and enables structured, type-driven logical transduction between them.</p> <p>Internally, Agentics is implemented as a Pydantic model. It holds:     \u2022   atype: a reference to the Pydantic class shared by all objects in the list.     \u2022   states: a list of Pydantic instances, each validated to be of type atype.     \u2022   tools: a list of tools (CrewAI or Langchain) to be used for transduction</p> <pre><code>from typing import Type, List\nfrom pydantic import BaseModel, Field\n\nclass Agentics(BaseModel):\n    atype: Type[BaseModel] = Field(\n        ..., \n        description=\"The shared Pydantic type for all elements in the list.\"\n    )\n    states: List[BaseModel] = []\n    tools: Optional[List[Any]] = Field(\n        None,\n        description=\"List of tools to be used by the agent\"\n    )\n    ...\n</code></pre>"},{"location":"agentics/#atypes","title":"Atypes","text":"<p>Agentics types are dynamic as they can be modified at run time while ensuring coherent semantics of the data they represent. To this aim, their Pydantic type is represented by the aslot, that can be assigned and modified at runtime. </p> <p>Any subclass of BaseModel (i.e. any possible Pydantic Type) can be used as an atype as long as it is serializable.</p> <pre><code>from agentics.core.agentics import Agentics as AG\nfrom pydantic import BaseModel\n\nclass Movie(BaseModel):\n    title: str\n    genre: str\n    description:str\n\nmovies = AG(atype=Movie)\nmovies.states.append(Movie(title=\"Superman\"))\nprint(movies)\n</code></pre>"},{"location":"agentics/#importing-csv-and-jsonl","title":"Importing CSV and JSONL","text":"<p>Agentics states can be initialized loaded and saved to .csv or .jsonl files. AType will be automatically generated from the column names (after they will be normalized as needed for valid pydantic fields), with all attributes set to strings.</p> <pre><code>from pydantic import BaseModel\nfrom agentics.core.agentics import Agentics as AG\n\n\n# Load CSV automatically acquiring atype\norders = AG.from_csv(\"data/orders.csv\")\n\n# Note that atype contains only strings.\nprint(orders.atype)\norders.to_csv(\"data/orders_copy.csv\")\n\n# Load Jsonl automatically acquiring atype. \norders = AG.from_jsonl(\"data/orders.jsonl\")\n\n# Note that atype contains integers fields not only strings.\nprint(orders.atype)\norders.to_jsonl(\"data/orders_copy.jsonl\")\n</code></pre> <p>If atype is provided, the file must contain fields that match the attributes defined in atype for them to be acquired, otherwise they'll be set to null. Providing explicit atype is recommedend to have more control on the types of the attributes, which will be otherwise set to string, and consistency on the column names and attribute matching. In addition, it is a convenient way to narrow down the number of attributes required for the task.</p> <pre><code># Load from CSV providing custom type (Only matching column names will be inferred)\norders = AG.from_csv(\"data/orders.csv\", atype = Order)\nprint(orders.atype)\n\n#note that states contains only the attribites in atype, others have been filtered out\norders.pretty_print()\nAG.to_csv(\"data/orders_filtered.jsonl\")\n</code></pre>"},{"location":"agentics/#rebind","title":"Rebind","text":"<p>Agentic types are mutable, and can be modified dynamically, by assigning a new atype</p> <pre><code>movies = AG.from_csv(\"data/orders.csv\")\nprint(movies.atype)\n\nclass MovieReview(Movie):\n    review:str\n    quality_score:Optional[int] = Field(None,description=\"The quality of the movies in a scale 0 to 10\")\n\nmovies.rebind_atype(MovieReview)\nprint(movies.states[0])\n</code></pre> <p>You can also modify and rebind an exiting Agentic. Similarly can also remove attributes. The following code is equivalent to the code before</p> <pre><code>movies = AG.from_csv(\"data/orders.csv\")\nmovies.add_attribute(\"review\",str)\nmovies.add_attribute(\"quality_score\",int,description=\"The quality of the movies in a scale 0 to 10\")\nprint(movies[0])\nmovies.subset_atype(\"title\",\"genre\",\"description\")\nprint(movies[0]) ## note that movies[0] is equivalent to \n</code></pre>"},{"location":"agentics/#transduction-between-agentics","title":"\ud83c\udfaf Transduction between Agentics","text":"<p>In addition to Transducible Functions syntax, AGs enables built in transduction. This was the preferred syntax for AG 1.0 and it is still supported by Agentics 2.0.</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom agentics import AG\nfrom typing import Optional\n\nclass Answer(BaseModel):\n    answer: Optional[str] = None\n    justification: Optional[str] = None\n    confidence: Optional[float] = None\n\nasync def main():\n    input_questions = [\n        \"What is the capital of Italy?\",\n        \"What is the best F1 team in history?\",\n    ]\n\n    answers = await (AG(atype=Answer) \\\n                     &lt;&lt; input_questions)\n\n    answers.pretty_print()\n\nasyncio.run(main())\n</code></pre>"},{"location":"agentics/#reference-code","title":"Reference code","text":"<p>explore this example</p>"},{"location":"agentics/#see-next-transduction","title":"See Next: Transduction","text":"<p>Wrapping pydantic types into Agentics provides them with the ability to perform transduction, as described in the next section</p>"},{"location":"core_concepts/","title":"\ud83e\udde0 Core Concepts","text":"<p>Agentics is built around a small set of concepts that work together:</p> <ul> <li>Pydantic types \u2013 how you describe structured data  </li> <li>Transducible functions \u2013 LLM-powered, type-safe transformations  </li> <li>Typed state containers (AGs) \u2013 collections of typed rows/documents  </li> <li>Logical Transduction Algebra (LTA) \u2013 the formal backbone  </li> <li>Map\u2013Reduce \u2013 the execution pattern for large workloads  </li> </ul> <p>This page gives you the mental model you need before diving into code.</p>"},{"location":"core_concepts/#1-pydantic-types-describing-structured-data","title":"1. Pydantic Types: Describing Structured Data \ud83d\udcd0","text":"<p>At the heart of Agentics is the idea that everything is a type.</p> <p>You describe your data using Pydantic models:</p> <pre><code>from pydantic import BaseModel\n\nclass Product(BaseModel):\n    id: str | None = None\n    title: str | None = None\n    description: str | None = None\n    price: float | None = None\n</code></pre> <p>These models serve three roles:</p> <ol> <li>Schema \u2013 they define the fields, types, and optionality  </li> <li>Validation \u2013 they validate inputs and outputs at runtime  </li> <li>Contract \u2013 they act as the contract between your code and the LLM  </li> </ol> <p>In Agentics, any LLM-powered transformation is expressed as:</p> <p>\u201cGiven a <code>Source</code> type, produce a <code>Target</code> type.\u201d</p> <p>Instead of prompt engineering around raw strings, you define transformations between types.</p>"},{"location":"core_concepts/#2-transducible-functions-typed-llm-transformations","title":"2. Transducible Functions: Typed LLM Transformations \u2699\ufe0f","text":"<p>A transducible function is the core abstraction in Agentics.</p> <p>Informally:</p> <p>A transducible function is an LLM-backed function that maps inputs of type <code>Source</code> to outputs of type <code>Target</code> under a set of instructions and constraints.</p> <p>Conceptually:</p> <pre><code>Target &lt;&lt; Source\n</code></pre> <p>Example:</p> <pre><code>from pydantic import BaseModel\n\nclass Review(BaseModel):\n    text: str\n\nclass ReviewSummary(BaseModel):\n    sentiment: str\n    summary: str\n</code></pre> <p>A transducible function might be:</p> <pre><code>fn: (Review) -&gt; ReviewSummary\n</code></pre> <p>with instructions like:</p> <p>\u201cGiven a review, detect its sentiment (positive/negative/neutral) and produce a one-sentence summary.\u201d</p> <p>Key properties:</p> <ul> <li>Typed I/O \u2013 the function is bound to <code>Source</code> and <code>Target</code> Pydantic models.  </li> <li>Single Source of Truth for Instructions \u2013 instructions live alongside the function definition.  </li> <li>LLM-Agnostic \u2013 the function describes what to transform; the underlying model can change.  </li> <li>Composable \u2013 functions can be chained, branched, or merged into larger workflows.</li> </ul> <p>You don\u2019t call the LLM directly; you call the transducible function, which manages LLM calls, validation, retries, and evidence tracking.</p>"},{"location":"core_concepts/#3-typed-state-containers-ags-working-with-collections","title":"3. Typed State Containers (AGs): Working with Collections \ud83d\uddc2\ufe0f","text":"<p>Transformations rarely happen on a single object. You typically work with collections of items (rows, documents, events, etc.).</p> <p>Agentics introduces typed state containers (AG) to:</p> <ul> <li>Hold a collection of instances of a given Pydantic type  </li> <li>Preserve that type information across operations  </li> <li>Provide a uniform interface for Map\u2013Reduce, filtering, joining, etc.</li> </ul> <p>Conceptually, you can think of an <code>AG[Source]</code> like a type-aware table:</p> <pre><code>AG[Review]\n  \u251c\u2500 row 0: Review(text=\"\u2026\")\n  \u251c\u2500 row 1: Review(text=\"\u2026\")\n  \u2514\u2500 row n: Review(text=\"\u2026\")\n</code></pre> <p>Applying a transducible function <code>(Review) -&gt; ReviewSummary</code> over an <code>AG[Review]</code> conceptually yields an <code>AG[ReviewSummary]</code>.</p> <p>Typed state containers give you:</p> <ul> <li>Clarity \u2013 you always know what type you\u2019re holding.  </li> <li>Safety \u2013 operations can check types and schemas instead of guessing.  </li> <li>Composability \u2013 containers can flow between functions and stages.</li> </ul> <p>You can think of state containers as the data plane of Agentics.</p> <p>Note: The name Agentics is derived as a legacy from the first version of Agentics, in which data models and transformations were blended into the same object. By introducing transducible functions as first class citizens, Agentics 2.0 uses AGs primarily as a data structure, although it is still possible to use them directly for transformations. See agentics v1.0 documentation to learn more. </p>"},{"location":"core_concepts/#4-logical-transduction-algebra-lta-the-formal-backbone","title":"4. Logical Transduction Algebra (LTA): The Formal Backbone \ud83d\udcda","text":"<p>Transducible functions and typed states are not just coding patterns; they are backed by a formal framework called Logical Transduction Algebra (LTA).</p> <p>You do not need to understand the full mathematics to use Agentics, but the intuition is important:</p> <ul> <li> <p>Transductions as Morphisms   Each transducible function is treated as a morphism between types: <code>Source \u27f6 Target</code>.</p> </li> <li> <p>Composability   If you have <code>f: A \u27f6 B</code> and <code>g: B \u27f6 C</code>, then you can form a composite transduction <code>g \u2218 f: A \u27f6 C</code>. Agentics gives you a practical way to do this over LLM-based functions.</p> </li> <li> <p>Explainability &amp; Evidence   Because transductions are modeled as structured mappings, Agentics can track which fields and which steps contributed to the final outputs. This underpins evidence tracking and traceability.</p> </li> </ul> <p>In short:</p> <p>LTA provides the theoretical foundation for why your pipelines are composable and explainable, even though they are powered by probabilistic models.</p>"},{"location":"core_concepts/#5-mapreduce-scaling-transductions","title":"5. Map\u2013Reduce: Scaling Transductions \ud83d\ude80","text":"<p>Once you have:</p> <ul> <li>Typed collections (<code>AG[Source]</code>), and  </li> <li>Typed transformations (<code>Source -&gt; Target</code>),</li> </ul> <p>you need a way to run these at scale. Agentics uses a familiar pattern: Map\u2013Reduce.</p>"},{"location":"core_concepts/#51-map-phase-amap","title":"5.1 Map Phase (<code>amap</code>)","text":"<p>The map phase applies a transducible function to each element (or batch) of a collection.</p> <p>Conceptually:</p> <pre><code>list[Source]  --amap(f)--&gt;  list[Target]\n</code></pre> <p>Where <code>f: Source -&gt; Target</code>.</p> <p>Properties:</p> <ul> <li>Parallelizable \u2013 each element can be processed independently.  </li> <li>Asynchronous \u2013 <code>amap</code> is designed for async I/O and concurrent execution.  </li> <li>Typed In/Out \u2013 both input and output containers carry their types.</li> </ul> <p>Typical use cases:</p> <ul> <li>Extracting structured info from documents  </li> <li>Enriching rows with LLM-derived attributes  </li> <li>Normalizing or cleaning text fields at scale  </li> </ul>"},{"location":"core_concepts/#52-reduce-phase-areduce","title":"5.2 Reduce Phase (<code>areduce</code>)","text":"<p>The reduce phase aggregates a collection back into a smaller structure (often a single summary or global view).</p> <pre><code>list[Target]  --areduce(g)--&gt;  GlobalSummary\n</code></pre> <p>Where <code>g</code> is a transducible function or aggregation operation that takes many items and produces fewer (often one).</p> <p>Examples:</p> <ul> <li>Summarizing a whole dataset into a report object  </li> <li>Producing global statistics or flags  </li> <li>Clustering and relation induction </li> </ul> <p>Map\u2013Reduce in Agentics is a logical pattern, not tied to any specific infrastructure:</p> <ul> <li><code>amap</code> = \u201capply a typed transformation to many items\u201d  </li> <li><code>areduce</code> = \u201caggregate many results into fewer structured outputs\u201d</li> </ul> <p>Together, they define how large-scale reasoning workflows are expressed in Agentics.</p>"},{"location":"core_concepts/#6-how-the-concepts-fit-together","title":"6. How the Concepts Fit Together \ud83d\udd17","text":"<p>A typical workflow looks like this:</p> <ol> <li> <p>Define your types    Use Pydantic to describe your raw data (<code>Source</code>) and desired outputs (<code>Target</code>, <code>Report</code>, etc.).</p> </li> <li> <p>Define transducible functions    For each logical step, define a transducible function:    extraction \u2192 normalization \u2192 classification \u2192 enrichment \u2192 summarization.</p> </li> <li> <p>Load data into typed state containers (Optional)    Wrap your dataset into a container such as <code>AG[Source]</code>.     You can also use simple python lists of objects of the intended type. </p> </li> <li> <p>Apply Map\u2013Reduce </p> </li> <li>Use <code>amap</code> to apply transducible functions over the collection. </li> <li> <p>Use <code>areduce</code> to build global summaries or reports.</p> </li> <li> <p>Rely on LTA properties    Because everything is a typed transduction, you can:  </p> </li> <li>Compose steps cleanly,  </li> <li>Trace outputs back to inputs,  </li> <li>Reason about structure and invariants in your pipeline.</li> </ol>"},{"location":"core_concepts/#7-summary","title":"7. Summary \u2705","text":"<ul> <li>Pydantic types give you schemas and validation.  </li> <li>Transducible functions turn LLM calls into typed, reusable transformations.  </li> <li>Typed state containers hold collections of those types with clear semantics.  </li> <li>Logical Transduction Algebra (LTA) explains why these transformations compose and remain interpretable.  </li> <li>Map\u2013Reduce provides the pattern for scaling these transductions to large datasets.</li> </ul> <p>From here, you can explore:</p> <ul> <li>\ud83d\udc49 Transducible Functions for concrete examples of defining and using transducible functions</li> <li>\ud83d\udc49 <code>types_and_states.md</code> for data modeling patterns  </li> <li>\ud83d\udc49 <code>mapreduce.md</code> to see how large-scale execution works in practice  </li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#what-is-agentics","title":"What is agentics?","text":"<p>Agentics is a lightweight, Python-native framework for building structured, agentic workflows over tabular or JSON-based data using Pydantic types and transduction logic. Designed to work seamlessly with large language models (LLMs), Agentics enables users to define input and output schemas as structured types and apply declarative, composable transformations, called transductions across data collections. Inspired by a low-code design philosophy, Agentics is ideal for rapidly prototyping intelligent systems that require structured reasoning and interpretable outputs over both structured and unstructured data. </p>"},{"location":"getting_started/#installation","title":"Installation","text":"<ul> <li>Clone the repository</li> </ul> <pre><code>  git clone git@github.com:IBM/agentics.git\n  cd agentics\n</code></pre> <ul> <li>Install uv (skip if available) </li> </ul> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Other installation options here</p> <ul> <li>Install the dependencies</li> </ul> <pre><code>uv sync\n# Source the environment (optional, you can skip this and prepend uv run to the later lines)\nsource .venv/bin/activate # bash/zsh \ud83d\udc1a\nsource .venv/bin/activate.fish # fish \ud83d\udc1f\n</code></pre>"},{"location":"getting_started/#set-environment-variables","title":"\ud83c\udfaf Set Environment Variables","text":"<p>Create a <code>.env</code> file in the root directory with your environment variables. See <code>.env.sample</code> for an example.</p> <p>Set Up LLM provider, Chose one of the following: </p>"},{"location":"getting_started/#openai","title":"OpenAI","text":"<ul> <li>Obtain API key from OpenAI</li> <li><code>OPENAI_API_KEY</code> - Your OpenAI APIKey</li> <li><code>OPENAI_MODEL_ID</code> - Your favorute model, default to openai/gpt-4</li> </ul>"},{"location":"getting_started/#ollama-local","title":"Ollama (local)","text":"<ul> <li>Download and install Ollama</li> <li>Download a Model. You should use a model that support reasoning and fit your GPU. So smaller are preferred.  <pre><code>ollama pull ollama/deepseek-r1:latest\n</code></pre></li> <li>\"OLLAMA_MODEL_ID\" - ollama/gpt-oss:latest (better quality), ollama/deepseek-r1:latest (smaller)</li> </ul>"},{"location":"getting_started/#ibm-watsonx","title":"IBM WatsonX:","text":"<ul> <li> <p><code>WATSONX_APIKEY</code> - WatsonX API key</p> </li> <li> <p><code>MODEL</code>  - watsonx/meta-llama/llama-3-3-70b-instruct (or alternative supporting function call)</p> </li> </ul>"},{"location":"getting_started/#google-gemini-offer-free-api-key","title":"Google Gemini (offer free API key)","text":"<ul> <li> <p><code>WATSONX_APIKEY</code> - WatsonX API key</p> </li> <li> <p><code>MODEL</code>  - watsonx/meta-llama/llama-3-3-70b-instruct (or alternative supporting function call)</p> </li> </ul>"},{"location":"getting_started/#vllm-need-dedicated-gpu-server","title":"VLLM (Need dedicated GPU server):","text":"<ul> <li>Set up your local instance of VLLM</li> <li><code>VLLM_URL</code> - http://base_url:PORT/v1</li> <li><code>VLLM_MODEL_ID</code> - Your model id (e.g. \"hosted_vllm/meta-llama/Llama-3.3-70B-Instruct\" )</li> </ul>"},{"location":"getting_started/#test-installation","title":"Test Installation","text":"<p>test hello world example (need to set up llm credentials first)</p> <pre><code>python python examples/hello_world.py\npython examples/self_transduction.py\npython examples/agentics_web_search_report.py\n</code></pre>"},{"location":"getting_started/#hello-world","title":"Hello World","text":"<pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\nfrom agentics.core.transducible_functions import Transduce, transducible\n\n\nclass Movie(BaseModel):\n    movie_name: Optional[str] = None\n    description: Optional[str] = None\n    year: Optional[int] = None\n\n\nclass Genre(BaseModel):\n    genre: Optional[str] = Field(None, description=\"e.g., comedy, drama, action\")\n\nmovie = Movie(movie_name=\"The Godfather\")\n\ngenre = await (Genre &lt;&lt; Movie)(movie)\n</code></pre>"},{"location":"getting_started/#installation-details","title":"Installation details","text":"PoetryPythonuvuvx \ud83c\udfc3\ud83c\udffdConda <p>Install poetry (skip if available)</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Clone and install agentics</p> <pre><code>poetry install\nsource $(poetry env info --path)/bin/activate \n</code></pre> <p>Ensure you have Python 3.11+ \ud83d\udea8.</p> <pre><code>python --version\n</code></pre> <ul> <li> <p>Create a virtual environment with Python's built in <code>venv</code> module. In linux, this  package may be required to be installed with the Operating System package manager.     <pre><code>python -m venv .venv\n</code></pre></p> </li> <li> <p>Activate the virtual environment</p> </li> </ul> <ul> <li>Ensure <code>uv</code> is installed. <pre><code>command -v uv &gt;/dev/null &amp;&amp;  curl -LsSf https://astral.sh/uv/install.sh | sh\n# It's recommended to restart the shell afterwards\nexec $SHELL\n</code></pre></li> <li><code>uv venv --python 3.11</code></li> <li><code>uv pip install ./agentics</code> or <code>uv add ./agentics</code> (recommended)</li> </ul> <p>This is a way to run agentics temporarily or quick tests</p> <ul> <li>Ensure <code>uv</code> is installed. <pre><code>command -v uv &gt;/dev/null &amp;&amp;  curl -LsSf https://astral.sh/uv/install.sh | sh\n# It's recommended to restart the shell afterwards\nexec $SHELL\n</code></pre></li> <li>uvx --verbose --from ./agentics ipython</li> </ul> <ol> <li> <p>Create a conda environment:    <pre><code>conda create -n agentics python=3.11\n</code></pre>    In this example the name of the environment is <code>agetnics</code> but you can change    it to your personal preference.</p> </li> <li> <p>Activate the environment     <pre><code>conda activate agentics\n</code></pre></p> </li> <li>Install <code>agentics</code> from a folder or git reference     <pre><code>pip install ./agentics\n</code></pre></li> </ol>"},{"location":"getting_started/#bashzsh","title":"Bash/Zsh","text":"<p><code>source .venv/bin/activate</code></p>"},{"location":"getting_started/#fish","title":"Fish","text":"<p><code>source .venv/bin/activate.fish</code></p>"},{"location":"getting_started/#vscode","title":"VSCode","text":"<p>Press <code>F1</code> key and start typing <code>&gt; Select python</code> and select <code>Select Python Interpreter</code></p> <ul> <li>Install the package     <pre><code>python -m pip install ./agentics\n</code></pre></li> </ul>"},{"location":"getting_started/#documentation","title":"Documentation","text":"<p>This documentation page is written using Mkdocs.  You can start the server to visualize this interactively. <pre><code>mkdocs serve\n</code></pre> After started, documentation will be available here http://127.0.0.1:8000/</p>"},{"location":"transducible_functions/","title":"\u2699\ufe0f Transducible Functions","text":"<p>Transducible functions are the workhorse of Agentics. They turn \u201ccall this LLM with a prompt\u201d into:</p> <p>A typed, explainable transformation <code>T: X \u2192 Y</code> with guarantees about how each output field was produced.</p> <p>This document explains what transducible functions are, how they work in Agentics, and how to use them in practice \u2014 including dynamic generation and compositional patterns using the <code>&lt;&lt;</code> operator.</p>"},{"location":"transducible_functions/#1-what-is-a-transducible-function","title":"1. What Is a Transducible Function?","text":"<p>Formally, a transducible function \\(T : X \\to Y\\) is an explainable function that satisfies:</p> <ol> <li> <p>Totality    For every valid input \\(x \\in \\llbracket X \\rrbracket\\), the function produces a valid output of type \\(Y\\).  </p> <p>No silent failures: the function always returns some well-typed <code>Y</code>.</p> </li> <li> <p>Local Evidence    Each output slot \\(y_i\\) is computed only from its evidence subset \\(\\mathcal{E}_i(x)\\).  </p> <p>No field is generated \u201cfrom nowhere\u201d: if <code>subject</code> appears in the output, we know which inputs and instructions it depended on.</p> </li> <li> <p>Slot-Level Provenance    The mapping between input and output slots is explicit:    [    \\mathcal{T}(y_i) = \\mathcal{E}_i    ]    This induces a bipartite graph between input slots and output slots, which acts as the explainability trace of the transduction.</p> </li> </ol> <p>Intuitively:</p> <ul> <li>An ordinary function only tells you \u201chere is the output.\u201d </li> <li>A transducible function also tells you \u201chere is the output, and here is exactly which inputs I used and why\u201d</li> </ul> <p>Transducible functions extend normal functions with structural transparency at the slot level.</p>"},{"location":"transducible_functions/#2-source-and-target-types-x-and-y","title":"2. Source and Target Types (X and Y) \ud83d\udcd0","text":"<p>Agentics uses Pydantic models to represent the input type <code>X</code> and the output type <code>Y</code>.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass UserMessage(BaseModel):\n    content: Optional[str] = None\n\nclass Email(BaseModel):\n    \"\"\"A simple email schema.\"\"\"\n    to: Optional[str] = Field(None, description=\"Recipient name or email address.\")\n    subject: Optional[str] = None\n    body: Optional[str] = None\n</code></pre> <ul> <li><code>UserMessage</code> is our Source type (<code>X</code>).</li> <li><code>Email</code> is our Target type (<code>Y</code>).</li> </ul> <p>Recommendation In transduction scenarios, it is often useful to declare fields as <code>Optional[...] = None</code>. This gives the LLM the ability to say \u201cI don\u2019t have enough evidence for this field\u201d by leaving it <code>null</code>, instead of hallucinating content.</p> <p>The transducible function we will define next will transform exactly one <code>UserMessage</code> into one <code>Email</code> (and later, we\u2019ll see how to scale to lists).</p>"},{"location":"transducible_functions/#3-defining-transducible-functions","title":"3. Defining Transducible Functions","text":"<p>In Agentics, transducible functions are <code>async</code> Python functions that:</p> <ul> <li>Accept exactly one instance of the source type <code>X</code> as input.</li> <li>Return exactly one instance of the target type <code>Y</code>.</li> </ul> <p>They can be defined in two main ways:</p> <ol> <li>Using the <code>@transducible()</code> decorator on an async Python function.</li> <li>Dynamically generating them from source and target types (e.g., via builders or the <code>&lt;&lt;</code> operator), with instructions and parameters.</li> </ol> <p>This section starts with the decorator pattern and then moves to dynamic generation and composition.</p>"},{"location":"transducible_functions/#4-the-transducible-decorator","title":"4. The <code>@transducible()</code> Decorator","text":"<p>The decorator turns an ordinary async function into a transducible function. When decorated with <code>@transducible()</code>, your function can return either:</p> <ul> <li>A concrete instance of the target type <code>Y</code> (pure Python logic), or</li> <li>A special <code>Transduce</code> object wrapping an instance of the source type <code>X</code>, which means:</li> </ul> <p>\u201cSend this source state to the LLM and let the model generate the target type <code>Y</code>.\u201d</p>"},{"location":"transducible_functions/#41-example-hybrid-llm-programmatic-logic","title":"4.1 Example: Hybrid LLM + Programmatic Logic","text":"<pre><code>import re\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom agentics.core.transducible_functions import transducible, Transduce\n\nclass UserMessage(BaseModel):\n    content: Optional[str] = None\n\nclass Email(BaseModel):\n    to: Optional[str] = None\n    subject: Optional[str] = None\n    body: Optional[str] = None\n</code></pre>"},{"location":"transducible_functions/#llm-driven-email-generation","title":"LLM-driven email generation","text":"<pre><code>@transducible()\nasync def write_email_with_llm(state: UserMessage) -&gt; Email:\n    \"\"\"Write a full email about the provided content.\n    The LLM is allowed to elaborate and make up reasonable details.\"\"\"\n    # Optionally mutate or pre-process state here\n    return Transduce(state)\n</code></pre> <p>Here, <code>Transduce(state)</code> signals:</p> <ul> <li>\u201cUse the transduction engine with this <code>UserMessage</code> as evidence.\u201d</li> <li>The LLM will generate an <code>Email</code> instance, respecting the schema.</li> </ul>"},{"location":"transducible_functions/#programmatic-email-extraction-no-llm","title":"Programmatic email extraction (no LLM)","text":"<pre><code>@transducible()\nasync def write_email_programmatically(state: UserMessage) -&gt; Email:\n    pattern = r\"^(Hi|Dear|Hello|Hey)\\s+([^,]+),\\s*(.+)$\"\n    match = re.match(pattern, state.content or \"\")\n    if match:\n        greeting, name, body = match.groups()\n        return Email(to=name, body=body)\n    # Not enough evidence \u2192 return an empty Email\n    return Email()\n</code></pre> <p>This function is also transducible, even if it does not call any LLM:</p> <ul> <li>It still respects totality: for any <code>UserMessage</code> it returns a valid <code>Email</code>.</li> <li>Local evidence is explicit: <code>to</code> and <code>body</code> come directly from <code>content</code>.</li> <li>Slot-level provenance is trivial: each field maps to a substring in <code>content</code>.</li> </ul> <p>Because both functions are transducible, they can be composed, traced, and plugged into Map\u2013Reduce pipelines in exactly the same way.</p>"},{"location":"transducible_functions/#5-executing-transducible-functions","title":"5. Executing Transducible Functions","text":"<p>You call a transducible function just like any other async function:</p> <pre><code>message = UserMessage(\n    content=\"Hi Lisa, I made great progress with the new release of Agentics 2.0\"\n)\n\ntarget1 = await write_email_with_llm(message)\ntarget2 = await write_email_programmatically(message)\n</code></pre>"},{"location":"transducible_functions/#51-example-outputs","title":"5.1 Example Outputs","text":"<p><code>target1</code> (LLM-based) may return something like:</p> <pre><code>{\n  \"to\": \"Lisa\",\n  \"subject\": \"Update on Agentics 2.0\",\n  \"body\": \"Hi Lisa,\\n\\nI wanted to share some exciting news about the new release of Agentics 2.0. Over the past week, I made great progress on the features we discussed...\\n\\nBest regards,\\n[Your Name]\"\n}\n</code></pre> <p><code>target2</code> (programmatic) will deterministically return:</p> <pre><code>{\n  \"to\": \"Lisa\",\n  \"subject\": null,\n  \"body\": \"I made great progress with the new release of Agentics 2.0\"\n}\n</code></pre> <p>A few important observations:</p> <ul> <li>The LLM output is stochastic: repeated calls may differ in style, but must remain logically transducible and semantically aligned with the evidence.</li> <li>The programmatic output is deterministic and brittle (it depends strictly on the regex).</li> <li>In practice, you combine both patterns:</li> <li>Use deterministic logic when the pattern is simple and strict.</li> <li>Use LLM-based transduction when structure is fixed but content is open-ended.</li> </ul>"},{"location":"transducible_functions/#6-dynamic-generation-composition-of-transducible-functions","title":"6. Dynamic Generation &amp; Composition of Transducible Functions","text":"<p>Beyond the decorator, Agentics lets you generate and compose transducible functions dynamically using the <code>&lt;&lt;</code> operator and helpers such as <code>With(...)</code>.</p> <p>Conceptually, the operator implements:</p> <p>Typed transduction construction <code>Y &lt;&lt; X</code> means: \u201cBuild a transducible function that maps from type <code>X</code> to type <code>Y</code>.\u201d </p> <p>You can use it with:</p> <ul> <li>Types (<code>Y &lt;&lt; X</code>),</li> <li>Existing transducible functions (<code>Y &lt;&lt; f</code>), and</li> <li>Configuration wrappers (<code>Y &lt;&lt; With(X, ...)</code>).</li> </ul>"},{"location":"transducible_functions/#61-minimal-setup","title":"6.1 Minimal Setup","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\nfrom agentics.core.transducible_functions import With\n\nclass GenericInput(BaseModel):\n    content: Optional[str] = None\n\nclass Email(BaseModel):\n    \"\"\"Email generated from a generic input.\"\"\"\n    to: Optional[str] = Field(None, description=\"Recipient of the email.\")\n    subject: Optional[str] = None\n    body: Optional[str] = None\n</code></pre>"},{"location":"transducible_functions/#62-dynamic-generation-with-type-function","title":"6.2 Dynamic Generation with <code>&lt;&lt;</code> (Type \u2192 Function)","text":"<p>The simplest form of dynamic generation is:</p> <pre><code>write_mail = Email &lt;&lt; GenericInput\n</code></pre> <p>This constructs a transducible function:</p> <pre><code>write_mail: GenericInput \u2192 Email\n</code></pre> <p>Usage:</p> <pre><code>input_state = GenericInput(\n    content=\"Write a news story on Zoran Mandani winning the election in NYC and send it to Alfio\"\n)\n\nmail = await write_mail(input_state)\nprint(mail.model_dump_json(indent=2))\n</code></pre> <p>Here, <code>Email &lt;&lt; GenericInput</code> tells Agentics:</p> <ul> <li>\u201cCreate an LLM-backed transducible function that maps a <code>GenericInput</code> into an <code>Email</code>.\u201d</li> <li>The default instructions depend on your configuration and global defaults (or you can refine them via <code>With</code>, shown below).</li> </ul>"},{"location":"transducible_functions/#63-composing-transductions-with","title":"6.3 Composing Transductions with <code>&lt;&lt;</code>","text":"<p>You can build multi-step pipelines by composing transducible functions and types using <code>&lt;&lt;</code>.</p> <p>Suppose we want to add a summary step on top of the email:</p> <pre><code>class Summary(BaseModel):\n    summary_text: Optional[str] = None\n</code></pre>"},{"location":"transducible_functions/#631-two-step-composition","title":"6.3.1. Two-step composition","text":"<pre><code>input_state = GenericInput(\n    content=\"Write news story on Zoran Mandani winning the election in NYC and send it to Alfio\"\n)\n\nwrite_mail = Email &lt;&lt; GenericInput             # GenericInput \u2192 Email\nsummary_from_email = Summary &lt;&lt; Email          # Email \u2192 Summary\n\n# Composition by function application\nmail = await write_mail(input_state)\nsummary = await summary_from_email(mail)\n\nprint(mail.model_dump_json(indent=2))\nprint(summary.model_dump_json(indent=2))\n</code></pre>"},{"location":"transducible_functions/#632-composition-via-on-functions","title":"6.3.2. Composition via <code>&lt;&lt;</code> on functions","text":"<p>You can also let <code>&lt;&lt;</code> perform the composition directly:</p> <pre><code># Compose Summary on top of write_mail\nsummary_composite_1 = Summary &lt;&lt; write_mail   # GenericInput \u2192 Summary\n\nsummary1 = await summary_composite_1(input_state)\nprint(summary1.model_dump_json(indent=2))\n</code></pre> <p>Or inline:</p> <pre><code>summary_composite_2 = Summary &lt;&lt; (Email &lt;&lt; GenericInput)\nsummary2 = await summary_composite_2(input_state)\nprint(summary2.model_dump_json(indent=2))\n</code></pre> <p>In all cases, the pipeline is:</p> <pre><code>GenericInput  \u2192  Email  \u2192  Summary\n</code></pre> <p>but you can choose whether to:</p> <ul> <li>Write the steps explicitly, or</li> <li>Build them into a single composed transducible function.</li> </ul>"},{"location":"transducible_functions/#64-using-with-for-configured-dynamic-transduction","title":"6.4 Using <code>With(...)</code> for Configured Dynamic Transduction","text":"<p>The <code>With(...)</code> helper lets you attach instructions and options to dynamic transductions.</p> <p>Example: first generate an email, then rewrite it into a compact summary:</p> <pre><code>from agentics.core.transducible_functions import With\n\nclass Summary(BaseModel):\n    summary_text: Optional[str] = None\n\n# A basic dynamic transduction\nwrite_mail = Email &lt;&lt; GenericInput\n\n# A configured transduction: Email \u2192 Summary\nsummarize = Summary &lt;&lt; With(\n    Email,\n    instructions=\"Rewrite the email into a concise summary.\",\n    enforce_output_type=True,\n    verbose_transduction=False,\n)\n\ninput_state = GenericInput(\n    content=\"Zoran Mandani won the election in NYC. Draft a message to the press list.\"\n)\n\nmail = await write_mail(input_state)\nsummary = await summarize(mail)\n\nprint(mail.model_dump_json(indent=2))\nprint(summary.model_dump_json(indent=2))\n</code></pre> <p>Here:</p> <ul> <li><code>With(Email, ...)</code> tells Agentics: \u201cWhen you see an <code>Email</code> as input, apply these instructions and guarantees to produce a <code>Summary</code>.\u201d</li> <li><code>enforce_output_type=True</code> strengthens validation so outputs must conform to <code>Summary</code>.</li> <li><code>verbose_transduction=False</code> keeps logs / metadata minimal (implementation-dependent).</li> </ul> <p>Because <code>Summary &lt;&lt; With(Email, ...)</code> is still a transducible function, you can compose it further, call it on lists, or plug it into Map\u2013Reduce.</p>"},{"location":"transducible_functions/#65-adding-explanations-with-with","title":"6.5. Adding explanations with <code>With(...)</code>","text":"<p>You can also ask for a structured explanation of the classification:</p> <pre><code>classify_genre = Genre &lt;&lt; With(\n    Movie,\n    provide_explanation=True,\n)\n\ngenre, explanation = await classify_genre(movie)\nprint(genre.model_dump_json(indent=2))\nprint(explanation.model_dump_json(indent=2))\n</code></pre> <p>Here, <code>provide_explanation=True</code> configures the dynamic transduction so that:</p> <ul> <li>The first output is the typed <code>Genre</code>.</li> <li>The second output is an explanation object (typically another Pydantic model),   capturing why the classifier picked that genre.</li> </ul> <p>This pattern generalizes:</p> <ul> <li><code>With(..., provide_explanation=True)</code> can be used with other source/target pairs.</li> <li>Explanations can be logged, inspected, or surfaced in UI as transparent justification for the model\u2019s decision.</li> </ul>"},{"location":"transducible_functions/#7-mapreduce-scaling-transducible-functions","title":"7. Map\u2013Reduce: Scaling Transducible Functions \ud83d\ude80","text":"<p>When wrapped by <code>@transducible()</code> or created dynamically with <code>&lt;&lt;</code>, transducible functions are overloaded to accept lists of <code>X</code> as well. When called this way, they return a corresponding list of <code>Y</code>:</p> <pre><code>messages = [\n    UserMessage(content=\"Hi John, I made great progress with Agentics.\"),\n    UserMessage(content=\"Hi , I fixed the last blocking bug in the pipeline.\"),\n]\n\nemails = await write_email_with_llm(messages)\n</code></pre> <p>Under the hood, Agentics uses an asynchronous Map operation:</p> <ul> <li>Conceptually: <code>amap(write_email_with_llm, messages) -&gt; list[Email]</code></li> <li>Each element is processed independently, enabling concurrency and parallelism.</li> <li>This pattern scales to batch inference, dataset scans, and large evidence extraction tasks.</li> </ul> <p>Later, you can combine this with Reduce operations (e.g., summarizing all emails into a single report), forming full Map\u2013Reduce pipelines over typed states.</p>"},{"location":"transducible_functions/#8-evidence-provenance-and-explainability","title":"8. Evidence, Provenance, and Explainability","text":"<p>Because transducible functions are defined over explicit types and carry evidence subsets, Agentics can:</p> <ul> <li>Track which input fields contributed to each output field.</li> <li>Represent this as a bipartite graph between input and output slots.</li> <li>Attach this trace as metadata to your states (depending on your Agentics configuration).</li> </ul> <p>For example, in the email examples:</p> <ul> <li><code>Email.to</code> is mapped to (a span inside) <code>UserMessage.content</code>.</li> <li><code>Email.subject</code> may depend on the entire <code>content</code>.</li> <li><code>Email.body</code> is mostly grounded in <code>content</code>, plus stylistic priors from instructions.</li> </ul> <p>This is critical when you:</p> <ul> <li>Need auditable LLM behavior.  </li> <li>Want to debug why a particular field was generated.  </li> <li>Need to enforce \u201cno hallucination from outside these inputs\u201d policies.</li> </ul>"},{"location":"transducible_functions/#9-when-to-create-a-new-transducible-function","title":"9. When to Create a New Transducible Function","text":"<p>In a real system, you\u2019ll typically end up with many small, focused transducible functions instead of one giant one.</p> <p>Good reasons to define a separate transducible function:</p> <ul> <li>You\u2019re doing a logically distinct step:</li> <li>e.g., extract entities, normalize names, classify intent, summarize conversation.</li> <li>You want to test and benchmark that step independently.</li> <li>You expect to reuse it across pipelines.</li> <li>You need different instructions, constraints, or safety properties for that stage.</li> </ul> <p>Think of transducible functions as the operators of your Logical Transduction Algebra.</p>"},{"location":"transducible_functions/#10-summary","title":"10. Summary \u2705","text":"<ul> <li>A transducible function is a typed, explainable mapping <code>T: X \u2192 Y</code> with:</li> <li>Totality, Local Evidence, and Slot-Level Provenance.</li> <li>In Agentics:</li> <li>Inputs and outputs are modeled as Pydantic types (<code>X</code>, <code>Y</code>).</li> <li>You can define transducible functions via:<ul> <li>The <code>@transducible()</code> decorator,</li> <li>Dynamic builders like <code>make_transducible_function</code>, and</li> <li>The <code>&lt;&lt;</code> operator (with or without <code>With(...)</code>).</li> </ul> </li> <li>Functions can be purely programmatic, purely LLM-based, or hybrid.</li> <li>Transducible functions:</li> <li>Scale from single calls to batch Map\u2013Reduce workloads.</li> <li>Expose structured explainability traces for each output field.</li> <li>Compose into robust, interpretable, large-scale reasoning pipelines.</li> </ul> <p>From here you can explore:</p> <ul> <li><code>core_concepts.md</code> \u2013 the broader mental model (types, states, LTA, Map\u2013Reduce).  </li> <li><code>mapreduce.md</code> \u2013 how Agentics orchestrates large-scale transductions over typed state containers.  </li> <li><code>types_and_states.md</code> \u2013 how to design good schemas and manage collections of states.</li> </ul>"}]}