{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udf10 Agentics","text":"<p>Agentics is a lightweight, Python-native framework for building structured and massively parallel agentic workflows using Pydantic models and transducible functions. </p>"},{"location":"#documentation-overview","title":"\ud83d\udcda Documentation Overview","text":""},{"location":"#core-documentation","title":"Core Documentation","text":"<ul> <li> <p>Getting Started \ud83d\ude80   Install Agentics, set up your environment, and run your first transducible function over a small dataset.</p> </li> <li> <p>Core Concepts \ud83e\udde0   Pydantic types, transducible functions, typed state containers, Logical Transduction Algebra (LTA), and Map\u2013Reduce.</p> </li> <li> <p>Transducible Functions \u2699\ufe0f   How to define, configure, and execute transducible functions.   Understanding dynamic generation and composition of transducible functions, batch processing, and provenance of generation.</p> </li> <li> <p>Map-Reduce Operations \ud83d\udd01   Scaling transducible functions with map and reduce operations, batch processing patterns, and best practices.</p> </li> <li> <p>Semantic Operators \ud83d\udd0d   High-level declarative API for data transformations using natural language. Includes <code>sem_map</code>, <code>sem_filter</code>, <code>sem_agg</code>, and more LOTUS-style operations.</p> </li> <li> <p>Agentics (AG) \ud83e\uddec   Working with <code>AG</code> typed state containers, loading data from JSON/CSV/DataFrames, and preserving type information across the pipeline.</p> </li> </ul>"},{"location":"#advanced-topics","title":"Advanced Topics","text":"<ul> <li> <p>Performance Optimization \u26a1   Batch size tuning, persisting intermediate results, performance optimization strategies, performance benchmarking, error handling, and best practices.</p> </li> <li> <p>Tool Integration \ud83d\udd0c   Using MCP tools, tool usage patterns, custom tools, and best practices.</p> </li> </ul>"},{"location":"#tutorials-examples","title":"Tutorials &amp; Examples","text":"<ul> <li> <p>Logical Transduction Algebra \ud83d\udd01   Interactive tutorial: Chaining transducible functions, branching, fan-in/fan-out patterns, and building reusable pipeline components.</p> </li> <li> <p>Map-Reduce Tutorial \ud83d\ude80   Interactive tutorial: Using <code>amap</code> and <code>areduce</code> for large-scale runs, batching strategies, handling failures, and performance considerations.</p> </li> <li> <p>Examples &amp; Use Cases \ud83d\udcd8   End-to-end examples: text-to-SQL, data extraction and enrichment, classification, document workflows, evaluation pipelines, and more.</p> </li> </ul>"},{"location":"#how-to-cite-agentics","title":"How to Cite Agentics","text":"<ul> <li>References \ud83d\udcda   Academic papers and research that form the foundation of Agentics, including transduction algebra, agentic AI, and applications.</li> </ul>"},{"location":"#glossary","title":"\ud83d\udcd6 Glossary","text":"<p>AG (Agentics) Short for \"Agentics\". A typed state container that wraps a list of Pydantic objects, enabling structured transductions. Used as <code>AG[Type]</code> or simply <code>AG(atype=Type)</code>. The recommended way to work with collections of typed data.</p> <p>Agentics The full name of the framework and the class name for typed state containers. In code, typically imported and used as <code>AG</code> for brevity.</p> <p>Transducible Function A typed, explainable function that maps inputs of type <code>Source</code> to outputs of type <code>Target</code>. Defined using the <code>@transducible()</code> decorator or dynamically with the <code>&lt;&lt;</code> operator. Guarantees totality, local evidence, and slot-level provenance.</p> <p>Transduction The process of transforming data from one typed structure to another using LLM-powered reasoning. Unlike simple mapping, transduction preserves semantic relationships and provides explainability.</p> <p>Logical Transduction Algebra (LTA) The formal mathematical framework underlying Agentics. Treats transductions as morphisms between types, enabling composition, explainability, and stability guarantees.</p> <p><code>&lt;&lt;</code> Operator (Left Shift) The transduction operator. <code>Target &lt;&lt; Source</code> creates a transducible function that maps <code>Source</code> to <code>Target</code>. Can be used with types, instances, or existing functions for composition.</p> <p><code>With()</code> Function A helper that wraps a source type with configuration parameters. Used as <code>Target &lt;&lt; With(Source, instructions=\"...\", tools=[...])</code> to create configured transducible functions dynamically.</p> <p>TransductionResult A wrapper object returned when <code>provide_explanation=True</code>. Supports automatic unpacking into <code>(value, explanation)</code> tuples or single value assignment.</p> <p>AType Short for \"Agentics Type\". The Pydantic model class that defines the schema for all instances in an AG container. Accessed via <code>ag.atype</code>.</p> <p>Map-Reduce The execution pattern for scaling transductions. <code>amap</code> applies a function to each element in parallel; <code>areduce</code> aggregates results into a summary.</p> <p>MCP (Model Context Protocol) A standard protocol for exposing tools (web search, databases, APIs) to LLMs. Agentics supports MCP tools via the <code>tools</code> parameter.</p> <p>Evidence The subset of input fields that contributed to generating a specific output field. Tracked automatically to enable explainability and provenance.</p> <p>Slot A field in a Pydantic model. \"Slot-level provenance\" means tracking which input slots contributed to each output slot.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>This documentation page is written using Mkdocs.  You can start the server to visualize this interactively. <pre><code>mkdocs serve\n</code></pre> After started, documentation will be available here http://127.0.0.1:8000/</p>"},{"location":"agentics/","title":"AG (Agentics)","text":"<p>AG (short for \"Agentics\") objects are wrappers around lists of objects having the same Pydantic type. They are designed to enable async logical transduction among their instances. AG containers enable us to think about AI workflows in terms of structured data transformations rather than agent behaviors, knowledge, and tasks.</p>"},{"location":"agentics/#the-ag-class","title":"The AG Class","text":"<p>AG (Agentics) is a Python class that wraps a list of Pydantic objects and enables structured, type-driven logical transduction between them.</p> <p>Import and Usage: <pre><code>from agentics import AG  # Recommended: use AG alias\n\n# Create a typed container\nmovies = AG(atype=Movie)\n</code></pre></p> <p>Internally, Agentics is implemented as a Pydantic model. It holds:     \u2022   atype: a reference to the Pydantic class shared by all objects in the list.     \u2022   states: a list of Pydantic instances, each validated to be of type atype.     \u2022   tools: a list of tools (CrewAI or Langchain) to be used for transduction</p> <pre><code>from typing import Type, List\nfrom pydantic import BaseModel, Field\n\nclass Agentics(BaseModel):\n    atype: Type[BaseModel] = Field(\n        ..., \n        description=\"The shared Pydantic type for all elements in the list.\"\n    )\n    states: List[BaseModel] = []\n    tools: Optional[List[Any]] = Field(\n        None,\n        description=\"List of tools to be used by the agent\"\n    )\n    ...\n</code></pre>"},{"location":"agentics/#atypes","title":"Atypes","text":"<p>Agentics types are dynamic as they can be modified at run time while ensuring coherent semantics of the data they represent. To this aim, their Pydantic type is represented by the aslot, that can be assigned and modified at runtime. </p> <p>Any subclass of BaseModel (i.e. any possible Pydantic Type) can be used as an atype as long as it is serializable.</p> <pre><code>from agentics.core.agentics import Agentics as AG\nfrom pydantic import BaseModel\n\nclass Movie(BaseModel):\n    title: str\n    genre: str\n    description:str\n\nmovies = AG(atype=Movie)\nmovies.states.append(Movie(title=\"Superman\"))\nprint(movies)\n</code></pre>"},{"location":"agentics/#importing-csv-and-jsonl","title":"Importing CSV and JSONL","text":"<p>Agentics states can be initialized loaded and saved to .csv or .jsonl files. AType will be automatically generated from the column names (after they will be normalized as needed for valid pydantic fields), with all attributes set to strings.</p> <pre><code>from pydantic import BaseModel\nfrom agentics.core.agentics import Agentics as AG\n\n\n# Load CSV automatically acquiring atype\norders = AG.from_csv(\"data/orders.csv\")\n\n# Note that atype contains only strings.\nprint(orders.atype)\norders.to_csv(\"data/orders_copy.csv\")\n\n# Load Jsonl automatically acquiring atype. \norders = AG.from_jsonl(\"data/orders.jsonl\")\n\n# Note that atype contains integers fields not only strings.\nprint(orders.atype)\norders.to_jsonl(\"data/orders_copy.jsonl\")\n</code></pre> <p>If atype is provided, the file must contain fields that match the attributes defined in atype for them to be acquired, otherwise they'll be set to null. Providing explicit atype is recommedend to have more control on the types of the attributes, which will be otherwise set to string, and consistency on the column names and attribute matching. In addition, it is a convenient way to narrow down the number of attributes required for the task.</p> <pre><code># Load from CSV providing custom type (Only matching column names will be inferred)\norders = AG.from_csv(\"data/orders.csv\", atype = Order)\nprint(orders.atype)\n\n#note that states contains only the attribites in atype, others have been filtered out\norders.pretty_print()\nAG.to_csv(\"data/orders_filtered.jsonl\")\n</code></pre>"},{"location":"agentics/#rebind","title":"Rebind","text":"<p>Agentic types are mutable, and can be modified dynamically, by assigning a new atype</p> <pre><code>movies = AG.from_csv(\"data/orders.csv\")\nprint(movies.atype)\n\nclass MovieReview(Movie):\n    review:str\n    quality_score:Optional[int] = Field(None,description=\"The quality of the movies in a scale 0 to 10\")\n\nmovies.rebind_atype(MovieReview)\nprint(movies.states[0])\n</code></pre> <p>You can also modify and rebind an exiting Agentic. Similarly can also remove attributes. The following code is equivalent to the code before.</p> <pre><code>movies = AG.from_csv(\"data/orders.csv\")\nprint(movies[0])\nmovies.add_attribute(\"review\",str)\nmovies.add_attribute(\"quality_score\",int,description=\"The quality of the movies in a scale 0 to 10\")\nprint(movies[0])\nmovies.subset_atype(\"title\",\"genre\",\"description\")\nprint(movies[0]) ## note that movies[0] is a shorthand for movies.states[0]\n</code></pre>"},{"location":"agentics/#transduction-between-agentics","title":"\ud83c\udfaf Transduction between Agentics","text":"<p>In addition to Transducible Functions syntax, AGs enables built in transduction. This was the preferred syntax for AG 1.0 and it is still supported by Agentics 2.0.</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom agentics import AG\nfrom typing import Optional\n\nclass Answer(BaseModel):\n    answer: Optional[str] = None\n    justification: Optional[str] = None\n    confidence: Optional[float] = None\n\nasync def main():\n    input_questions = [\n        \"What is the capital of Italy?\",\n        \"What is the best F1 team in history?\",\n    ]\n\n    answers = await (AG(atype=Answer) &lt;&lt; input_questions)\n\n    answers.pretty_print()\n\nasyncio.run(main())\n</code></pre>"},{"location":"agentics/#reference-code","title":"Reference code","text":"<p>See the examples directory for practical demonstrations of AG usage, including: - <code>hello_world.py</code> - Basic transduction example - <code>generate_tweets.py</code> - Content generation - <code>emotion_extractor.py</code> - Text analysis</p>"},{"location":"agentics/#go-to-index","title":"Go to Index","text":"<ul> <li>\ud83d\udc49 Index</li> </ul>"},{"location":"core_concepts/","title":"\ud83e\udde0 Core Concepts","text":"<p>Agentics is built around a small set of concepts that work together:</p> <ul> <li>Pydantic types \u2013 how you describe structured data  </li> <li>Transducible functions \u2013 LLM-powered, type-safe transformations  </li> <li>Typed state containers (AGs) \u2013 collections of typed rows/documents  </li> <li>Logical Transduction Algebra (LTA) \u2013 the formal backbone  </li> <li>Map\u2013Reduce \u2013 the programming model used to execute large-scale workloads</li> </ul> <p>This page gives you the mental model you need before diving into code.</p>"},{"location":"core_concepts/#1-pydantic-types-describing-structured-data","title":"1. Pydantic Types: Describing Structured Data \ud83d\udcd0","text":"<p>At the heart of Agentics is the idea that everything is a type.</p> <p>You describe your data using Pydantic models:</p> <pre><code>from pydantic import BaseModel\nfromp typing import Optional\n\nclass Product(BaseModel):\n    id: Optional[str] = None\n    title: Optional[str] = None\n    description: Optional[str] = None\n    price: Optional[float] = None\n</code></pre> <p>These models serve three roles:</p> <ol> <li>Schema \u2013 they define the fields, types, and optionality  </li> <li>Validation \u2013 they validate inputs and outputs at runtime  </li> <li>Contract \u2013 they act as the contract between your code and the LLM  </li> </ol> <p>In Agentics, any LLM-powered transformation is expressed as:</p> <p>\u201cGiven a <code>Source</code> type, produce a <code>Target</code> type.\u201d</p> <p>Instead of prompt engineering around raw strings, you define transformations between types.</p>"},{"location":"core_concepts/#2-transducible-functions-typed-llm-transformations","title":"2. Transducible Functions: Typed LLM Transformations \u2699\ufe0f","text":"<p>A transducible function is the core abstraction in Agentics.</p> <p>Informally:</p> <p>A transducible function is an LLM-backed function that maps inputs of type <code>Source</code> to outputs of type <code>Target</code> under a set of instructions and constraints.</p> <p>Conceptually:</p> <pre><code>Target &lt;&lt; Source\n</code></pre> <p>Example:</p> <pre><code>from pydantic import BaseModel\n\nclass Review(BaseModel):\n    text: Optional[str] = None\n\nclass ReviewSummary(BaseModel):\n    sentiment: Optional[str] = None\n    summary: Optional[str] = None\n</code></pre> <p>A transducible function might be:</p> <pre><code>fn: Review -&gt; ReviewSummary\n</code></pre> <p>with instructions like:</p> <p>\u201cGiven a review, detect its sentiment (positive/negative/neutral) and produce a one-sentence summary.\u201d</p> <p>Key properties:</p> <ul> <li>Typed I/O \u2013 the function is bound to <code>Source</code> and <code>Target</code> Pydantic models.  </li> <li>Single Source of Truth for Instructions \u2013 instructions live alongside the function definition.  </li> <li>LLM-Agnostic \u2013 the function describes what to transform; the underlying model can change.  </li> <li>Composable \u2013 functions can be chained, branched, or merged into larger workflows.</li> </ul> <p>You don\u2019t call the LLM directly; you call the transducible function, which manages LLM calls, validation, retries, and evidence tracking.</p>"},{"location":"core_concepts/#3-typed-state-containers-ag-working-with-collections","title":"3. Typed State Containers (AG): Working with Collections \ud83d\uddc2\ufe0f","text":"<p>Transformations rarely happen on a single object. You typically work with collections of items (rows, documents, events, etc.).</p> <p>Agentics introduces typed state containers (called AG, short for \"Agentics\") to:</p> <ul> <li>Hold a collection of instances of a given Pydantic type</li> <li>Preserve that type information across operations</li> <li>Provide a uniform interface for Map\u2013Reduce, filtering, joining, etc.</li> </ul> <p>Conceptually, you can think of an <code>AG[Source]</code> like a type-aware table:</p> <pre><code>AG[Review]\n  \u251c\u2500 row 0: Review(text=\"\u2026\")\n  \u251c\u2500 row 1: Review(text=\"\u2026\")\n  \u2514\u2500 row n: Review(text=\"\u2026\")\n</code></pre> <p>Applying a transducible function <code>Review -&gt; ReviewSummary</code> over an <code>AG</code> with atype <code>Review</code> conceptually yields an <code>AG</code> of type <code>ReviewSummary</code>. </p> <p>Typed state containers give you:</p> <ul> <li>Clarity \u2013 you always know what type you're holding.</li> <li>Safety \u2013 operations can check types and schemas instead of guessing.</li> <li>Composability \u2013 containers can flow between functions and stages.</li> </ul> <p>You can think of state containers (AGs) as the data plane of Agentics.</p> <pre><code>from agentics import AG  # Recommended alias\n\nmovies = AG(atype=Movie)  # Create a typed container\n</code></pre> <p>Historical Note: In Agentics 1.0, data models and transformations were blended into the same object. Agentics 2.0 separates concerns by introducing transducible functions as first-class citizens, while AG containers focus on data management. The v1.0 API is still supported for backward compatibility.</p>"},{"location":"core_concepts/#4-logical-transduction-algebra-lta-the-formal-backbone","title":"4. Logical Transduction Algebra (LTA): The Formal Backbone \ud83d\udcda","text":"<p>Transducible functions and typed states are not just coding patterns; they are backed by a formal framework called Logical Transduction Algebra (LTA).</p> <p>You do not need to understand the full mathematics to use Agentics, but the intuition is important:</p> <ul> <li> <p>Transductions as Morphisms   Each transducible function is treated as a morphism between types: <code>Source \u27f6 Target</code>.</p> </li> <li> <p>Composability   If you have <code>f: A \u27f6 B</code> and <code>g: B \u27f6 C</code>, then you can form a composite transduction <code>g \u2218 f: A \u27f6 C</code>. Agentics gives you a practical way to do this over LLM-based functions.</p> </li> <li> <p>Explainability &amp; Evidence   Because transductions are modeled as structured mappings, Agentics can track which fields and which steps contributed to the final outputs. This underpins evidence tracking and traceability.</p> </li> </ul> <p>In short:</p> <p>LTA provides the theoretical foundation for why your pipelines are composable and explainable, even though they are powered by probabilistic models.</p>"},{"location":"core_concepts/#5-mapreduce-scaling-transductions","title":"5. Map\u2013Reduce: Scaling Transductions \ud83d\ude80","text":"<p>Once you have:</p> <ul> <li>Typed collections (<code>AG[Source]</code>) and  </li> <li>Typed transformations (<code>Source -&gt; Target</code>),</li> </ul> <p>you need a way to run these at scale. Agentics uses a familiar pattern: Map\u2013Reduce.</p>"},{"location":"core_concepts/#51-map-phase-amap","title":"5.1 Map Phase (<code>amap</code>)","text":"<p>The map phase applies a transducible function to each element (or batch) of a collection.</p> <p>Conceptually:</p> <pre><code>list[Source]  --amap(f)--&gt;  list[Target]\n</code></pre> <p>Where <code>f: Source -&gt; Target</code>.</p> <p>Properties:</p> <ul> <li>Parallelizable \u2013 each element can be processed independently.  </li> <li>Asynchronous \u2013 <code>amap</code> is designed for async I/O and concurrent execution.  </li> <li>Typed In/Out \u2013 both input and output containers carry their types.</li> </ul> <p>Typical use cases:</p> <ul> <li>Extracting structured info from documents  </li> <li>Enriching rows with LLM-derived attributes  </li> <li>Normalizing or cleaning text fields at scale  </li> </ul>"},{"location":"core_concepts/#52-reduce-phase-areduce","title":"5.2 Reduce Phase (<code>areduce</code>)","text":"<p>The reduce phase aggregates a collection back into a smaller structure (often a single summary or global view).</p> <pre><code>list[Target]  --areduce(g)--&gt;  GlobalSummary\n</code></pre> <p>Where <code>g</code> is a transducible function or aggregation operation that takes many items and produces fewer (often one).</p> <p>Examples:</p> <ul> <li>Summarizing a whole dataset into a report object  </li> <li>Producing global statistics or flags  </li> <li>Clustering and relation induction </li> </ul> <p>Map\u2013Reduce in Agentics is a logical pattern, not tied to any specific infrastructure:</p> <ul> <li><code>amap</code> = \u201capply a typed transformation to many items\u201d  </li> <li><code>areduce</code> = \u201caggregate many results into fewer structured outputs\u201d</li> </ul> <p>Together, they define how large-scale reasoning workflows are expressed in Agentics.</p>"},{"location":"core_concepts/#6-how-the-concepts-fit-together","title":"6. How the Concepts Fit Together \ud83d\udd17","text":"<p>A typical workflow looks like this:</p> <ol> <li> <p>Define your types    Use Pydantic to describe your raw data (<code>Source</code>) and desired outputs (<code>Target</code>, <code>Report</code>, etc.).</p> </li> <li> <p>Define transducible functions    For each logical step, define a transducible function:    extraction \u2192 normalization \u2192 classification \u2192 enrichment \u2192 summarization.</p> </li> <li> <p>Load data into typed state containers (Optional)    Wrap your dataset into a container such as <code>AG[Source]</code>.     You can also use simple python lists of objects of the intended type. </p> </li> <li> <p>Apply Map\u2013Reduce </p> </li> <li>Use <code>amap</code> to apply transducible functions over the collection. </li> <li> <p>Use <code>areduce</code> to build global summaries or reports.</p> </li> <li> <p>Rely on LTA properties    Because everything is a typed transduction, you can:  </p> </li> <li>Compose steps cleanly,  </li> <li>Trace outputs back to inputs,  </li> <li>Reason about structure and invariants in your pipeline.</li> </ol>"},{"location":"core_concepts/#7-summary","title":"7. Summary \u2705","text":"<ul> <li>Pydantic types give you schemas and validation.  </li> <li>Transducible functions turn LLM calls into typed, reusable transformations.  </li> <li>Typed state containers hold collections of those types with clear semantics.  </li> <li>Logical Transduction Algebra (LTA) explains why these transformations compose and remain interpretable.  </li> <li>Map\u2013Reduce provides the pattern for scaling these transductions to large datasets.</li> </ul>"},{"location":"core_concepts/#next","title":"Next","text":"<ul> <li>\ud83d\udc49 Transducible Functions for concrete examples of defining and using transducible functions</li> </ul>"},{"location":"core_concepts/#go-to-index","title":"Go to Index","text":"<ul> <li>\ud83d\udc49 Index</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#what-is-agentics","title":"What is agentics?","text":"<p>Agentics is a lightweight, Python-native framework for building structured, agentic workflows over tabular or JSON-based data using Pydantic types and transduction logic. Designed to work seamlessly with large language models (LLMs), Agentics enables users to define input and output schemas as structured types and apply declarative, composable transformations, called transductions across data collections. Inspired by a low-code design philosophy, Agentics is ideal for rapidly prototyping intelligent systems that require structured reasoning and interpretable outputs over both structured and unstructured data. </p>"},{"location":"getting_started/#installation","title":"Installation","text":"<ul> <li>Clone the repository</li> </ul> <pre><code>  git clone git@github.com:IBM/agentics.git\n  cd agentics\n</code></pre> <ul> <li>Install uv (skip if available) </li> </ul> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Other installation options here</p> <ul> <li>Install the dependencies</li> </ul> <pre><code>uv sync\n# Source the environment (optional, you can skip this and prepend uv run to the later lines)\nsource .venv/bin/activate # bash/zsh \ud83d\udc1a\nsource .venv/bin/activate.fish # fish \ud83d\udc1f\n</code></pre>"},{"location":"getting_started/#environment-variables","title":"\ud83c\udfaf Environment Variables","text":"<p>Create a <code>.env</code> file in the root directory with your environment variables. See <code>.env.sample</code> for an example.</p> <p>Set up LLM provider, chose one of the following: </p>"},{"location":"getting_started/#openai","title":"OpenAI","text":"<ul> <li>Obtain API key from OpenAI</li> <li><code>OPENAI_API_KEY</code> - Your OpenAI APIKey</li> <li><code>OPENAI_MODEL_ID</code> - Selected model, default to openai/gpt-4</li> </ul>"},{"location":"getting_started/#ollama-local","title":"Ollama (local)","text":"<ul> <li>Download and install Ollama</li> <li>Download a model. You should use a model that support reasoning and fit your GPU. So smaller are preferred.  <pre><code>ollama pull ollama/deepseek-r1:latest\n</code></pre></li> <li>\"OLLAMA_MODEL_ID\" - ollama/gpt-oss:latest (better quality), ollama/deepseek-r1:latest (smaller)</li> </ul>"},{"location":"getting_started/#ibm-watsonx","title":"IBM WatsonX:","text":"<ul> <li> <p><code>WATSONX_APIKEY</code> - WatsonX API key</p> </li> <li> <p><code>MODEL</code>  - watsonx/meta-llama/llama-3-3-70b-instruct (or alternative supporting function call)</p> </li> </ul>"},{"location":"getting_started/#google-gemini-offers-free-api-key","title":"Google Gemini (offers free API key)","text":"<ul> <li> <p><code>GEMINI_API_KEY</code> - Your Google Gemini API key (get it from Google AI Studio)</p> </li> <li> <p><code>MODEL</code> - <code>gemini/gemini-1.5-pro</code> or <code>gemini/gemini-1.5-flash</code> (or other Gemini models supporting function calling)</p> </li> </ul>"},{"location":"getting_started/#vllm-need-dedicated-gpu-server","title":"VLLM (Need dedicated GPU server):","text":"<ul> <li>Set up your local instance of VLLM</li> <li><code>VLLM_URL</code> - http://base_url:PORT/v1</li> <li><code>VLLM_MODEL_ID</code> - Your model id (e.g. \"hosted_vllm/meta-llama/Llama-3.3-70B-Instruct\" )</li> </ul>"},{"location":"getting_started/#test-installation","title":"Test Installation","text":"<p>Test hello world example (need to set up llm credentials first)</p> <pre><code>python python examples/hello_world.py\npython examples/self_transduction.py\npython examples/agentics_web_search_report.py\n</code></pre>"},{"location":"getting_started/#hello-world","title":"Hello World","text":"<p>Transform boring product descriptions into viral tweets in just a few lines:</p> <pre><code>from pydantic import BaseModel, Field\nfrom agentics.core.transducible_functions import transducible, Transduce\n\nfrom typing import Optional\n\nclass ProductDescription(BaseModel):\n    name: Optional[str] = None\n    features: Optional[str] = None\n    price: Optional[float] = None\n\nclass ViralTweet(BaseModel):\n    tweet: Optional[str] = Field(None, description=\"Engaging tweet under 280 characters\")\n    hashtags: Optional[list[str]] = Field(None, description=\"3-5 relevant hashtags\")\n    hook: Optional[str] = Field(None, description=\"Attention-grabbing opening line\")\n\n@transducible()\nasync def generate_viral_tweet(product: ProductDescription) -&gt; ViralTweet:\n    \"\"\"Transform boring product descriptions into viral social media content.\"\"\"\n    return Transduce(product)\n\n# Transform a product into viral content\nproduct = ProductDescription(\n    name=\"Agentics Framework\",\n    features=\"Type-safe AI workflows with LLM-powered transductions\",\n    price=0.0  # Open source!\n)\n\ntweet = await generate_viral_tweet(product)\nprint(f\"\ud83d\udd25 {tweet.tweet}\")\nprint(f\"\ud83d\udcf1 {' '.join(tweet.hashtags)}\")\n</code></pre> <p>Output: <pre><code>\ud83d\udd25 Stop wrestling with unstructured LLM outputs! \ud83c\udfaf Agentics gives you type-safe AI workflows that just work. Build production-ready agents in minutes, not weeks. And it's FREE! \ud83d\ude80\n\ud83d\udcf1 #AI #OpenSource #Python #LLM #DevTools\n</code></pre></p>"},{"location":"getting_started/#alternative-using-operator","title":"Alternative: Using  <code>&lt;&lt;</code> Operator","text":"<p>For quick one-off transductions, use <code>&lt;&lt;</code> operator:</p> <pre><code>from pydantic import BaseModel\n\nfrom typing import Optional\n\nclass Product(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n\nclass Tweet(BaseModel):\n    content: Optional[str] = None\n\n# Create transduction on the fly\nmake_tweet = Tweet &lt;&lt; Product\n\nproduct = Product(\n    name=\"Agentics\",\n    description=\"Type-safe AI framework for Python\"\n)\n\ntweet = await make_tweet(product)\nprint(tweet.content)\n</code></pre> <p>This concise syntax is perfect for exploratory work and rapid prototyping!</p>"},{"location":"getting_started/#batch-processing-multiple-products","title":"Batch Processing: Multiple Products","text":"<p>Transducible functions automatically support batch processing. Process multiple products at once in parallel:</p> <pre><code>products = [\n    ProductDescription(\n        name=\"Agentics Framework\",\n        features=\"Type-safe AI workflows with LLM-powered transductions\",\n        price=0.0\n    ),\n    ProductDescription(\n        name=\"Smart Coffee Maker\",\n        features=\"AI-powered brewing with perfect temperature control\",\n        price=299.99\n    ),\n    ProductDescription(\n        name=\"Wireless Earbuds Pro\",\n        features=\"Active noise cancellation and 30-hour battery life\",\n        price=149.99\n    ),\n]\n\n# Automatically processes all products in parallel\ntweets = await generate_viral_tweet(products)\n\n# Display results\nfor product, tweet in zip(products, tweets):\n    print(f\"\\n\ud83d\udce6 Product: {product.name}\")\n    print(f\"\ud83d\udd25 Tweet: {tweet.tweet}\")\n    print(f\"\ud83d\udcf1 Tags: {' '.join(tweet.hashtags)}\")\n</code></pre> <p>Output: <pre><code>\ud83d\udce6 Product: Agentics Framework\n\ud83d\udd25 Tweet: Stop wrestling with unstructured LLM outputs! \ud83c\udfaf Agentics gives you type-safe AI workflows that just work. Build production-ready agents in minutes, not weeks. And it's FREE! \ud83d\ude80\n\ud83d\udcf1 Tags: #AI #OpenSource #Python #LLM #DevTools\n\n\ud83d\udce6 Product: Smart Coffee Maker\n\ud83d\udd25 Tweet: Wake up to perfection! \u2615 Our AI-powered coffee maker learns your taste and brews the perfect cup every time. Never settle for mediocre coffee again! \ud83e\udd16\n\ud83d\udcf1 Tags: #SmartHome #Coffee #AI #Tech #MorningRoutine\n\n\ud83d\udce6 Product: Wireless Earbuds Pro\n\ud83d\udd25 Tweet: Silence the world, amplify your music! \ud83c\udfa7 30 hours of pure audio bliss with active noise cancellation. Your commute just got an upgrade! \ud83d\udd0b\n\ud83d\udcf1 Tags: #Audio #Tech #Wireless #Music #Productivity\n</code></pre></p> <p>The same transducible function works seamlessly for both single items and batches\u2014no code changes needed!</p>"},{"location":"getting_started/#next","title":"Next","text":"<ul> <li>\ud83d\udc49 Core Concepts - Understanding the theoretical foundation</li> </ul>"},{"location":"getting_started/#go-to-index","title":"Go to Index","text":"<ul> <li>\ud83d\udc49 Index</li> </ul>"},{"location":"map_reduce/","title":"\ud83d\udd01 Map-Reduce Operations","text":"<p>Map-Reduce is the execution pattern for scaling transducible functions to large datasets. Agentics provides built-in support for both map (parallel transformation) and reduce (aggregation) operations over typed collections.</p>"},{"location":"map_reduce/#overview","title":"Overview","text":"<p>When you define a transducible function, it automatically supports both single-item and batch processing:</p> <pre><code>from pydantic import BaseModel\n\nclass UserMessage(BaseModel):\n    content: str\n\nclass Email(BaseModel):\n    to: str\n    subject: str\n    body: str\n\n@transducible()\nasync def write_email(message: UserMessage) -&gt; Email:\n    \"\"\"Convert a message into a professional email.\"\"\"\n    return Transduce(message)\n\n# Single item\nemail = await write_email(UserMessage(content=\"Hi John, great progress!\"))\n\n# Batch processing (automatic map)\nmessages = [\n    UserMessage(content=\"Hi John, I made great progress with Agentics.\"),\n    UserMessage(content=\"Hi, I fixed the last blocking bug in the pipeline.\"),\n]\nemails = await write_email(messages)  # Returns list[Email]\n</code></pre>"},{"location":"map_reduce/#the-map-operation","title":"The Map Operation","text":"<p>The map operation applies a transducible function to each element independently, enabling concurrency and parallelism.</p>"},{"location":"map_reduce/#how-map-works","title":"How Map Works","text":"<pre><code># Conceptually:\n# amap(write_email, messages) -&gt; list[Email]\n\n# Each element is processed independently\n# Results maintain the same order as inputs\n</code></pre>"},{"location":"map_reduce/#map-characteristics","title":"Map Characteristics","text":"Aspect Description Input Single item or list of items Output List of transformed items (one per input) Operation Independent transformation of each element Parallelization Fully parallel - elements processed concurrently Use Cases Enrichment, extraction, classification, normalization"},{"location":"map_reduce/#map-examples","title":"Map Examples","text":"<p>Example 1: Data Enrichment</p> <pre><code>class Product(BaseModel):\n    name: str\n    category: str\n\nclass EnrichedProduct(BaseModel):\n    name: str\n    category: str\n    description: str\n    keywords: list[str]\n\n@transducible()\nasync def enrich_product(product: Product) -&gt; EnrichedProduct:\n    \"\"\"Add description and keywords to product.\"\"\"\n    return Transduce(product)\n\n# Process entire catalog\nproducts = load_products()  # list[Product]\nenriched = await enrich_product(products)  # Parallel processing\n</code></pre> <p>Example 2: Text Classification</p> <pre><code>class Document(BaseModel):\n    text: str\n\nclass ClassifiedDocument(BaseModel):\n    text: str\n    category: str\n    confidence: float\n    tags: list[str]\n\n@transducible(batch_size=20)\nasync def classify_document(doc: Document) -&gt; ClassifiedDocument:\n    \"\"\"Classify document into categories.\"\"\"\n    return Transduce(doc)\n\ndocuments = load_documents(1000)\nclassified = await classify_document(documents)  # Processes in batches of 20\n</code></pre>"},{"location":"map_reduce/#the-reduce-operation","title":"The Reduce Operation","text":"<p>The reduce operation aggregates a collection of items into a single summary or consolidated result.</p>"},{"location":"map_reduce/#using-transduction_typeareduce","title":"Using <code>transduction_type=\"areduce\"</code>","text":"<p>Specify the transduction type to create a reduce operation:</p> <pre><code>from typing import List\n\nclass Review(BaseModel):\n    text: str\n    rating: int\n\nclass ReviewSummary(BaseModel):\n    overall_sentiment: str\n    average_rating: float\n    key_themes: List[str]\n    total_reviews: int\n\n@transducible(transduction_type=\"areduce\")\nasync def summarize_reviews(reviews: List[Review]) -&gt; ReviewSummary:\n    \"\"\"Aggregate multiple reviews into a single summary.\"\"\"\n    return Transduce(reviews)\n\n# Use it\nreviews = [\n    Review(text=\"Great product!\", rating=5),\n    Review(text=\"Good value for money\", rating=4),\n    Review(text=\"Not bad, could be better\", rating=3),\n]\n\nsummary = await summarize_reviews(reviews)\nprint(f\"Overall: {summary.overall_sentiment}\")\nprint(f\"Average: {summary.average_rating}\")\n</code></pre>"},{"location":"map_reduce/#reduce-characteristics","title":"Reduce Characteristics","text":"Aspect Description Input List of items Output Single aggregated result Operation Aggregation across all elements Parallelization Sequential or hierarchical Use Cases Summarization, statistics, consolidation, consensus"},{"location":"map_reduce/#common-reduce-patterns","title":"Common Reduce Patterns","text":"<p>Pattern 1: Summarization</p> <pre><code>class Document(BaseModel):\n    title: str\n    content: str\n\nclass ExecutiveSummary(BaseModel):\n    main_points: List[str]\n    conclusion: str\n    word_count: int\n\n@transducible(transduction_type=\"areduce\")\nasync def create_executive_summary(docs: List[Document]) -&gt; ExecutiveSummary:\n    \"\"\"Summarize multiple documents into key insights.\"\"\"\n    return Transduce(docs)\n</code></pre> <p>Pattern 2: Statistical Aggregation</p> <pre><code>class DataPoint(BaseModel):\n    value: float\n    category: str\n    timestamp: str\n\nclass Statistics(BaseModel):\n    mean: float\n    median: float\n    categories: List[str]\n    trend: str  # \"increasing\", \"decreasing\", \"stable\"\n\n@transducible(transduction_type=\"areduce\")\nasync def analyze_data(points: List[DataPoint]) -&gt; Statistics:\n    \"\"\"Compute statistics and identify trends.\"\"\"\n    return Transduce(points)\n</code></pre> <p>Pattern 3: Consensus Building</p> <pre><code>class Opinion(BaseModel):\n    author: str\n    stance: str\n    reasoning: str\n\nclass Consensus(BaseModel):\n    majority_view: str\n    key_arguments: List[str]\n    dissenting_views: List[str]\n    confidence: float\n\n@transducible(transduction_type=\"areduce\")\nasync def build_consensus(opinions: List[Opinion]) -&gt; Consensus:\n    \"\"\"Find consensus across multiple opinions.\"\"\"\n    return Transduce(opinions)\n</code></pre>"},{"location":"map_reduce/#dynamic-map-reduce-with-operator","title":"Dynamic Map-Reduce with <code>&lt;&lt;</code> Operator","text":"<p>Create map and reduce operations on the fly:</p>"},{"location":"map_reduce/#dynamic-map","title":"Dynamic Map","text":"<pre><code># Create a map function dynamically\nenrich = EnrichedProduct &lt;&lt; Product\n\nproducts = [Product(name=\"Widget\", category=\"Tools\"), ...]\nenriched = await enrich(products)  # Automatic map\n</code></pre>"},{"location":"map_reduce/#dynamic-reduce","title":"Dynamic Reduce","text":"<pre><code>from agentics import With\n\n# Create a reduce function on the fly\nsummarize = ReviewSummary &lt;&lt; With(\n    List[Review],\n    transduction_type=\"areduce\",\n    instructions=\"Analyze all reviews and provide comprehensive summary\"\n)\n\nsummary = await summarize(reviews)\n</code></pre>"},{"location":"map_reduce/#combining-map-and-reduce","title":"Combining Map and Reduce","text":"<p>Build complete Map-Reduce pipelines by chaining operations:</p> <pre><code># Step 1: Map - Extract insights from each document\nclass Document(BaseModel):\n    text: str\n\nclass Insight(BaseModel):\n    key_point: str\n    importance: int\n\n@transducible()\nasync def extract_insight(doc: Document) -&gt; Insight:\n    \"\"\"Extract key insight from a document.\"\"\"\n    return Transduce(doc)\n\n# Step 2: Reduce - Consolidate all insights\nclass Report(BaseModel):\n    top_insights: List[str]\n    overall_theme: str\n\n@transducible(transduction_type=\"areduce\")\nasync def consolidate_insights(insights: List[Insight]) -&gt; Report:\n    \"\"\"Consolidate insights into a final report.\"\"\"\n    return Transduce(insights)\n\n# Execute the pipeline\ndocuments = [Document(text=\"...\"), Document(text=\"...\"), ...]\ninsights = await extract_insight(documents)  # Map phase\nreport = await consolidate_insights(insights)  # Reduce phase\n</code></pre>"},{"location":"map_reduce/#multi-stage-pipeline-example","title":"Multi-Stage Pipeline Example","text":"<pre><code># Stage 1: Map - Clean and normalize\n@transducible()\nasync def clean_data(raw: RawData) -&gt; CleanData:\n    return Transduce(raw)\n\n# Stage 2: Map - Extract features\n@transducible()\nasync def extract_features(clean: CleanData) -&gt; Features:\n    return Transduce(clean)\n\n# Stage 3: Reduce - Aggregate statistics\n@transducible(transduction_type=\"areduce\")\nasync def compute_stats(features: List[Features]) -&gt; Statistics:\n    return Transduce(features)\n\n# Execute pipeline\nraw_data = load_raw_data()\nclean = await clean_data(raw_data)\nfeatures = await extract_features(clean)\nstats = await compute_stats(features)\n</code></pre>"},{"location":"map_reduce/#best-practices","title":"Best Practices","text":""},{"location":"map_reduce/#for-map-operations","title":"For Map Operations","text":"<ol> <li>Use appropriate batch sizes - Balance throughput and memory (see Optimization)</li> <li>Handle failures gracefully - Individual items can fail without stopping the batch</li> <li>Monitor progress - Use <code>verbose_transduction=True</code> for long-running operations</li> <li>Consider rate limits - Adjust batch size for API rate limits</li> </ol>"},{"location":"map_reduce/#for-reduce-operations","title":"For Reduce Operations","text":"<ol> <li>Keep reduce operations focused - Each reduce should have a clear aggregation goal</li> <li>Handle empty lists - Consider what happens when the input list is empty</li> <li>Use hierarchical reduction - For very large collections, reduce in stages</li> <li>Provide clear instructions - Help the LLM understand the aggregation logic</li> <li>Consider token limits - Large collections may exceed context windows</li> <li>Test with representative data - Ensure reduce logic works across different input sizes</li> </ol>"},{"location":"map_reduce/#general-best-practices","title":"General Best Practices","text":"<pre><code># Good: Clear separation of concerns\n@transducible()\nasync def extract(item: Raw) -&gt; Processed:\n    \"\"\"Map: Extract and normalize.\"\"\"\n    return Transduce(item)\n\n@transducible(transduction_type=\"areduce\")\nasync def summarize(items: List[Processed]) -&gt; Summary:\n    \"\"\"Reduce: Aggregate results.\"\"\"\n    return Transduce(items)\n\n# Execute\nprocessed = await extract(raw_items)\nsummary = await summarize(processed)\n</code></pre>"},{"location":"map_reduce/#performance-considerations","title":"Performance Considerations","text":""},{"location":"map_reduce/#batch-size-tuning","title":"Batch Size Tuning","text":"<pre><code># Small batches for complex operations\n@transducible(batch_size=5)\nasync def complex_analysis(item: Data) -&gt; Analysis:\n    return Transduce(item)\n\n# Large batches for simple operations\n@transducible(batch_size=30)\nasync def simple_extraction(item: Data) -&gt; Extract:\n    return Transduce(item)\n</code></pre>"},{"location":"map_reduce/#parallel-execution","title":"Parallel Execution","text":"<p>Map operations are automatically parallelized based on <code>batch_size</code>. For more control, see Optimization.</p>"},{"location":"map_reduce/#next","title":"Next","text":"<ul> <li>\ud83d\udc49 Map-Reduce Tutorial to see how large-scale execution works in practice</li> <li>\ud83d\udc49 Semantic Operators for performing data transformation tasks using natural language.</li> </ul>"},{"location":"map_reduce/#go-to-index","title":"Go to Index","text":"<ul> <li>\ud83d\udc49 Index</li> </ul>"},{"location":"optimization/","title":"\u26a1 Performance Optimization","text":"<p>Efficient batch processing and performance optimization are crucial for large-scale transductions. This guide covers strategies to maximize throughput, manage resources, and handle large datasets effectively.</p>"},{"location":"optimization/#understanding-batch-size","title":"Understanding Batch Size","text":"<p>The <code>batch_size</code> parameter controls how many items are processed concurrently. Choosing the right batch size is critical for balancing throughput, memory usage, and reliability.</p> <pre><code># Small batches - lower memory, more overhead\n@transducible(batch_size=5)\nasync def conservative_process(state: Item) -&gt; Result:\n    return Transduce(state)\n\n# Large batches - higher throughput, more memory\n@transducible(batch_size=25)\nasync def aggressive_process(state: Item) -&gt; Result:\n    return Transduce(state)\n</code></pre>"},{"location":"optimization/#choosing-the-right-batch-size","title":"Choosing the Right Batch Size","text":"Scenario Recommended Batch Size Reason Simple transformations (&lt; 1s each) 20-30 Maximize throughput Complex reasoning (&gt; 5s each) 5-10 Avoid timeout issues Large input/output objects 10-15 Manage memory usage Rate-limited APIs 5-15 Stay within limits Local LLM (Ollama) 1-5 Limited by GPU memory"},{"location":"optimization/#persisting-intermediate-results","title":"Persisting Intermediate Results","text":"<p>Use <code>persist_output</code> to save results incrementally, enabling recovery from failures:</p> <pre><code>@transducible(\n    batch_size=20,\n    persist_output=\"./output/processed_batches\"\n)\nasync def process_large_dataset(state: DataItem) -&gt; ProcessedItem:\n    \"\"\"Results saved after each batch completes.\"\"\"\n    return Transduce(state)\n\n# Process 10,000 items\nlarge_dataset = load_items(10000)\nresults = await process_large_dataset(large_dataset)\n\n# If interrupted, previously completed batches are saved\n# Resume by loading saved batches and processing remaining items\n</code></pre>"},{"location":"optimization/#file-structure","title":"File Structure","text":"<pre><code>output/processed_batches/\n\u251c\u2500\u2500 batch_0000.jsonl  # First 20 items\n\u251c\u2500\u2500 batch_0001.jsonl  # Next 20 items\n\u251c\u2500\u2500 batch_0002.jsonl  # And so on...\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"optimization/#monitoring-progress","title":"Monitoring Progress","text":"<p>Enable verbose logging to track batch processing:</p> <pre><code>@transducible(\n    batch_size=25,\n    verbose_transduction=True,  # Show progress\n    verbose_agent=False  # Hide detailed agent logs\n)\nasync def monitored_process(state: Item) -&gt; Result:\n    return Transduce(state)\n\n# Output shows:\n# Processing batch 1/40 (25 items)...\n# Processing batch 2/40 (25 items)...\n# ...\n</code></pre>"},{"location":"optimization/#performance-optimization-strategies","title":"Performance Optimization Strategies","text":""},{"location":"optimization/#1-adaptive-batch-sizing","title":"1. Adaptive Batch Sizing","text":"<p>Tune batch size based on item complexity:</p> <pre><code># Adaptive batching based on input size\ndef get_batch_size(items):\n    avg_size = sum(len(str(item)) for item in items) / len(items)\n    if avg_size &lt; 500:\n        return 25  # Small items\n    elif avg_size &lt; 2000:\n        return 15  # Medium items\n    else:\n        return 5  # Large items\n\nbatch_size = get_batch_size(dataset)\nprocess_fn = Result &lt;&lt; With(Item, batch_size=batch_size)\n</code></pre>"},{"location":"optimization/#2-field-specific-transduction","title":"2. Field-Specific Transduction","text":"<p>Only transduce the fields you need:</p> <pre><code>@transducible(\n    transduce_fields=[\"summary\", \"category\"],  # Only these fields\n    batch_size=30\n)\nasync def focused_transform(state: FullData) -&gt; PartialResult:\n    \"\"\"Faster by ignoring unnecessary fields.\"\"\"\n    return Transduce(state)\n</code></pre>"},{"location":"optimization/#3-reduce-token-usage-with-prompt-templates","title":"3. Reduce Token Usage with Prompt Templates","text":"<pre><code># Custom template to reduce token count\ncompact_template = \"\"\"\nInput: {input_data}\nTask: {instructions}\nOutput format: {output_schema}\n\"\"\"\n\n@transducible(\n    prompt_template=compact_template,\n    batch_size=40\n)\nasync def efficient_transform(state: Item) -&gt; Result:\n    return Transduce(state)\n</code></pre>"},{"location":"optimization/#4-parallel-processing-with-multiple-workers","title":"4. Parallel Processing with Multiple Workers","text":"<p>For extremely large datasets, consider splitting work across multiple processes:</p> <pre><code>import asyncio\nfrom concurrent.futures import ProcessPoolExecutor\n\nasync def process_chunk(chunk, process_fn):\n    \"\"\"Process a chunk of data.\"\"\"\n    return await process_fn(chunk)\n\nasync def parallel_process(dataset, process_fn, num_workers=4):\n    \"\"\"Split dataset across multiple workers.\"\"\"\n    chunk_size = len(dataset) // num_workers\n    chunks = [dataset[i:i+chunk_size] for i in range(0, len(dataset), chunk_size)]\n\n    tasks = [process_chunk(chunk, process_fn) for chunk in chunks]\n    results = await asyncio.gather(*tasks)\n\n    # Flatten results\n    return [item for chunk_result in results for item in chunk_result]\n</code></pre>"},{"location":"optimization/#performance-benchmarking","title":"Performance Benchmarking","text":"<p>Measure throughput for your specific use case:</p> <pre><code>import time\n\nasync def benchmark_transduction():\n    test_items = generate_test_data(100)\n\n    start = time.time()\n    results = await process_fn(test_items)\n    elapsed = time.time() - start\n\n    print(f\"Processed {len(results)} items in {elapsed:.2f}s\")\n    print(f\"Throughput: {len(results)/elapsed:.2f} items/sec\")\n    print(f\"Average time per item: {elapsed/len(results):.2f}s\")\n\nawait benchmark_transduction()\n</code></pre>"},{"location":"optimization/#profiling-memory-usage","title":"Profiling Memory Usage","text":"<pre><code>import tracemalloc\n\nasync def profile_memory():\n    tracemalloc.start()\n\n    # Your transduction\n    results = await process_fn(large_dataset)\n\n    current, peak = tracemalloc.get_traced_memory()\n    print(f\"Current memory: {current / 1024 / 1024:.2f} MB\")\n    print(f\"Peak memory: {peak / 1024 / 1024:.2f} MB\")\n\n    tracemalloc.stop()\n</code></pre>"},{"location":"optimization/#error-handling-retries","title":"Error Handling &amp; Retries","text":""},{"location":"optimization/#automatic-retries","title":"Automatic Retries","text":"<p>Configure retry behavior for transient failures:</p> <pre><code>@transducible(\n    max_retries=3,  # Retry up to 3 times\n    retry_delay=2.0,  # Wait 2 seconds between retries\n    batch_size=20\n)\nasync def resilient_process(state: Item) -&gt; Result:\n    return Transduce(state)\n</code></pre>"},{"location":"optimization/#graceful-degradation","title":"Graceful Degradation","text":"<p>Use optional fields to handle partial failures:</p> <pre><code>class RobustResult(BaseModel):\n    required_field: str\n    optional_field: Optional[str] = None  # May be None if extraction fails\n    confidence: Optional[float] = None\n\n@transducible(\n    batch_size=25,\n    allow_partial=True  # Continue even if some fields fail\n)\nasync def robust_transform(state: Item) -&gt; RobustResult:\n    return Transduce(state)\n</code></pre>"},{"location":"optimization/#batch-level-error-handling","title":"Batch-Level Error Handling","text":"<pre><code>async def process_with_error_handling(items):\n    results = []\n    failed = []\n\n    for batch in chunk_items(items, batch_size=20):\n        try:\n            batch_results = await process_fn(batch)\n            results.extend(batch_results)\n        except Exception as e:\n            print(f\"Batch failed: {e}\")\n            failed.extend(batch)\n\n    # Retry failed items with smaller batch size\n    if failed:\n        print(f\"Retrying {len(failed)} failed items...\")\n        retry_fn = Result &lt;&lt; With(Item, batch_size=5)\n        retry_results = await retry_fn(failed)\n        results.extend(retry_results)\n\n    return results\n</code></pre>"},{"location":"optimization/#best-practices","title":"Best Practices","text":"<ol> <li>Start with conservative batch sizes - Increase gradually based on benchmarks</li> <li>Monitor memory usage - Especially with large input/output objects</li> <li>Use persist_output for long-running jobs - Protect against interruptions</li> <li>Profile before optimizing - Measure to identify actual bottlenecks</li> <li>Consider API rate limits - Adjust batch size and concurrency accordingly</li> <li>Test with representative data - Performance varies with input complexity</li> <li>Use field-specific transduction - Only process what you need</li> <li>Enable progress monitoring - Track long-running operations</li> </ol>"},{"location":"optimization/#common-performance-issues","title":"Common Performance Issues","text":""},{"location":"optimization/#issue-high-memory-usage","title":"Issue: High Memory Usage","text":"<p>Symptoms: Process crashes or slows down with large datasets</p> <p>Solutions: - Reduce batch size - Use field-specific transduction - Process in chunks with persistence - Stream results instead of loading all at once</p>"},{"location":"optimization/#issue-slow-throughput","title":"Issue: Slow Throughput","text":"<p>Symptoms: Processing takes much longer than expected</p> <p>Solutions: - Increase batch size (if memory allows) - Reduce prompt complexity - Use faster LLM models - Optimize prompt templates - Consider parallel processing</p>"},{"location":"optimization/#issue-frequent-timeouts","title":"Issue: Frequent Timeouts","text":"<p>Symptoms: Many requests timeout or fail</p> <p>Solutions: - Reduce batch size - Increase timeout value - Simplify the transduction task - Use faster models - Check network connectivity</p>"},{"location":"optimization/#see-also","title":"See Also","text":"<ul> <li>\ud83d\udc49 Transducible Functions - Core concepts and basic usage</li> <li>\ud83d\udc49 Tool Integration - Using external tools</li> <li>\ud83d\udc49 Map-Reduce Tutorial - Large-scale execution patterns</li> <li>\ud83d\udc49 Index</li> </ul>"},{"location":"references/","title":"References","text":"<p>This page contains academic papers and research that form the foundation of Agentics. If you use Agentics in your research or project, please cite the relevant papers listed below. Each entry includes BibTeX citations for easy integration into your bibliography.</p> <p>Note: This list is actively maintained and updated as new papers are published.</p> <ol> <li>Transduction is All You Need for Structured Data Workflows (2025)</li> <li>Authors: Alfio Gliozzo, Naweed Khan, Christodoulos Constantinides, Nandana Mihindukulasooriya, Nahuel Defosse, Gaetano Rossiello, Junkyu Lee</li> <li> <p>URL: https://arxiv.org/abs/2508.15610    <pre><code>@article{gliozzo2025transduction,\n  title={Transduction is All You Need for Structured Data Workflows},\n  author={Gliozzo, Alfio and Khan, Naweed and Constantinides, Christodoulos and Mihindukulasooriya, Nandana and Defosse, Nahuel and Rossiello, Gaetano and Lee, Junkyu},\n  journal={arXiv preprint arXiv:2508.15610},\n  year={2025}\n}\n</code></pre></p> </li> <li> <p>Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets (2025)</p> </li> <li>Authors: Agostino Capponi, Alfio Gliozzo, Brian Zhu</li> <li> <p>URL: https://arxiv.org/abs/2512.02436    <pre><code>@article{capponi2025semantic,\n  title={Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets},\n  author={Capponi, Agostino and Gliozzo, Alfio and Zhu, Brian},\n  journal={arXiv preprint arXiv:2512.02436},\n  year={2025}\n}\n</code></pre></p> </li> <li> <p>DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance (2026)</p> </li> <li>Authors: Chunghyun Han, Alfio Gliozzo, Junkyu Lee, Agostino Capponi</li> <li>URL: https://arxiv.org/abs/2510.21117    <pre><code>@inproceedings{han2026daoai,\n  title={{DAO}-{AI}: Evaluating Collective Decision-Making through Agentic {AI} in Decentralized Governance},\n  author={Chunghyun Han and Alfio Gliozzo and Junkyu Lee and Agostino Capponi},\n  booktitle={AAAI'26 Workshop on Agentic AI in Financial Services},\n  year={2026},\n  url={https://arxiv.org/abs/2510.21117}\n}\n</code></pre></li> </ol>"},{"location":"semantic_operators/","title":"\ud83d\udd0d Semantic Operators","text":"<p>Semantic operators provide a high-level, declarative API for performing common data transformation tasks using natural language. Inspired by LOTUS-style semantic operations, these operators enable you to work with structured and unstructured data using LLM-powered transformations.</p>"},{"location":"semantic_operators/#overview","title":"Overview","text":"<p>Agentics semantic operators bridge the gap between traditional data manipulation (like pandas operations) and LLM-powered semantic understanding. Each operator accepts either an <code>AG</code> (Agentics) or a pandas <code>DataFrame</code> as input and returns the same type, making them easy to integrate into existing data pipelines.</p>"},{"location":"semantic_operators/#available-operators","title":"Available Operators","text":"Operator Description <code>sem_map</code> Map each record using a natural language instruction <code>sem_filter</code> Keep records that match a natural language predicate <code>sem_agg</code> Aggregate across all records (e.g., for summarization)"},{"location":"semantic_operators/#sem_map","title":"<code>sem_map</code>","text":"<p>Transform each record in your dataset according to natural language instructions, mapping source data to a target schema.</p>"},{"location":"semantic_operators/#signature","title":"Signature","text":"<pre><code>async def sem_map(\n    source: AG | pd.DataFrame,\n    target_type: Type[BaseModel] | str,\n    instructions: str,\n    merge_output: bool = True,\n    **kwargs,\n) -&gt; AG | pd.DataFrame\n</code></pre>"},{"location":"semantic_operators/#parameters","title":"Parameters","text":"<ul> <li><code>source</code> (<code>AG | pd.DataFrame</code>): Input data to be mapped</li> <li><code>target_type</code> (<code>Type[BaseModel] | str</code>): Target schema for the output</li> <li>If a Pydantic <code>BaseModel</code> subclass: used directly as the target type</li> <li>If a <code>str</code>: a Pydantic model is created dynamically with a single string field</li> <li><code>instructions</code> (<code>str</code>): Natural language description of how to transform the data</li> <li><code>merge_output</code> (<code>bool</code>, default=<code>True</code>): </li> <li><code>True</code>: Merge mapped fields back into original source records</li> <li><code>False</code>: Return only the mapped output</li> <li><code>**kwargs</code>: Additional arguments forwarded to <code>AG()</code> constructor (e.g., model configuration, batching)</li> </ul>"},{"location":"semantic_operators/#returns","title":"Returns","text":"<ul> <li><code>AG | pd.DataFrame</code>: <code>AG</code> or <code>DataFrame</code> that contains the transformed data following <code>target_type</code></li> </ul>"},{"location":"semantic_operators/#example-basic-mapping","title":"Example: Basic Mapping","text":"<pre><code>import pandas as pd\nfrom agentics.core.semantic_operators import sem_map\nfrom pydantic import BaseModel\n\n# Sample data\ndf = pd.DataFrame({\n    'review': [\n        'This product is amazing! Best purchase ever.',\n        'Terrible quality, broke after one day.',\n        'It works okay, nothing special.'\n    ]\n})\n\n# Define target schema\nclass Sentiment(BaseModel):\n    sentiment: Optional[str] = Field(None, description=\"The sentiment of the review (e.g., positive, negative, neutral)\")\n    confidence: Optional[float] = Field(None, description=\"Confidence score of the sentiment analysis btw 0 and 1\")\n\n# Map reviews to sentiment\nresult = await sem_map(\n    source=df,\n    target_type=Sentiment,\n    instructions=\"Analyze the sentiment of the review and provide a confidence score between 0 and 1.\"\n)\n\n# Output includes original 'review' column plus 'sentiment' and 'confidence' columns\n                                         review  sentiment  confidence\n 0  This product is amazing! Best purchase ever.  positive        0.85\n 1        Terrible quality, broke after one day.  negative        0.99\n 2               It works okay, nothing special.   neutral        0.85\n</code></pre>"},{"location":"semantic_operators/#example-string-based-target-type","title":"Example: String-based Target Type","text":"<pre><code># Using string target type for simpler cases\nresult = await sem_map(\n    source=df,\n    target_type=\"category\",\n    instructions=\"Classify the review into one of: positive, negative, neutral\"\n)\n</code></pre>"},{"location":"semantic_operators/#sem_filter","title":"<code>sem_filter</code>","text":"<p>Filter records based on a natural language predicate, keeping only those that satisfy the condition.</p>"},{"location":"semantic_operators/#signature_1","title":"Signature","text":"<pre><code>async def sem_filter(\n    source: AG | pd.DataFrame,\n    predicate_template: str,\n    **kwargs\n) -&gt; AG | pd.DataFrame\n</code></pre>"},{"location":"semantic_operators/#parameters_1","title":"Parameters","text":"<ul> <li><code>source</code> (<code>AG | pd.DataFrame</code>): Input data to be filtered</li> <li><code>predicate_template</code> (<code>str</code>): Natural language condition or LangChain-style template</li> <li>Can use <code>{field}</code> placeholders to reference source fields</li> <li>Or provide a plain text predicate</li> <li><code>**kwargs</code>: Additional arguments forwarded to <code>AG()</code> constructor</li> </ul>"},{"location":"semantic_operators/#returns_1","title":"Returns","text":"<ul> <li><code>AG | pd.DataFrame</code>: Filtered data containing only records that satisfy the predicate</li> </ul>"},{"location":"semantic_operators/#example-simple-predicate","title":"Example: Simple Predicate","text":"<pre><code>from agentics.core.semantic_operators import sem_filter\n\ndf = pd.DataFrame({\n    'product': ['Laptop', 'Phone', 'Tablet', 'Monitor'],\n    'description': [\n        'High-performance gaming laptop with RGB keyboard',\n        'Budget smartphone with basic features',\n        'Premium tablet with stylus support',\n        '4K monitor for professional work'\n    ]\n})\n\n# Filter for premium/high-end products\nresult = await sem_filter(\n    source=df,\n    predicate_template=\"The product is premium or high-end\"\n)\n\nprint(result)\n   product                                       description\n0   Laptop  High-performance gaming laptop with RGB keyboard\n1   Tablet                Premium tablet with stylus support\n2  Monitor                  4K monitor for professional work\n</code></pre>"},{"location":"semantic_operators/#example-template-based-filtering","title":"Example: Template-based Filtering","text":"<pre><code># Use field placeholders in the predicate\nresult = await sem_filter(\n    source=df,\n    predicate_template=\"The {product} described as '{description}' is suitable for gaming\"\n)\n</code></pre>"},{"location":"semantic_operators/#sem_agg","title":"<code>sem_agg</code>","text":"<p>Aggregate data across all records to produce a summary or consolidated output.</p>"},{"location":"semantic_operators/#signature_2","title":"Signature","text":"<pre><code>async def sem_agg(\n    source: AG | pd.DataFrame,\n    target_type: Type[BaseModel] | str,\n    instructions: str = None,\n    **kwargs,\n) -&gt; AG | pd.DataFrame\n</code></pre>"},{"location":"semantic_operators/#parameters_2","title":"Parameters","text":"<ul> <li><code>source</code> (<code>AG | pd.DataFrame</code>): Input data to be aggregated</li> <li><code>target_type</code> (<code>Type[BaseModel] | str</code>): Schema for the aggregated output</li> <li><code>instructions</code> (<code>str</code>, optional): Natural language description of the aggregation</li> <li><code>**kwargs</code>: Additional arguments forwarded to <code>AG()</code> constructor</li> </ul>"},{"location":"semantic_operators/#returns_2","title":"Returns","text":"<ul> <li><code>AG | pd.DataFrame</code>: Aggregated result (typically a single record or summary)</li> </ul>"},{"location":"semantic_operators/#example-summarization","title":"Example: Summarization","text":"<pre><code>from agentics.core.semantic_operators import sem_agg\nfrom pydantic import BaseModel\n\ndf = pd.DataFrame({\n    'review': [\n        'Great product, very satisfied!',\n        'Good quality but expensive',\n        'Not worth the price',\n        'Excellent, would buy again',\n        'Decent but has some issues'\n    ]\n})\n\nclass ReviewSummary(BaseModel):\n    overall_sentiment: str\n    key_themes: list[str]\n    recommendation: str\n\n# Aggregate all reviews into a summary\nresult = await sem_agg(\n    source=df,\n    target_type=ReviewSummary,\n    instructions=\"Summarize all reviews, identify key themes, and provide an overall recommendation\"\n)\n\nprint(result)\n# Returns a single record with aggregated insights\n</code></pre>"},{"location":"semantic_operators/#example-statistical-summary","title":"Example: Statistical Summary","text":"<pre><code>class Statistics(BaseModel):\n    total_count: int\n    positive_count: int\n    negative_count: int\n    average_sentiment: str\n\nresult = await sem_agg(\n    source=df,\n    target_type=Statistics,\n    instructions=\"Count total reviews, positive reviews, negative reviews, and determine average sentiment\"\n)\n</code></pre>"},{"location":"semantic_operators/#best-practices","title":"Best Practices","text":""},{"location":"semantic_operators/#1-choose-the-right-operator","title":"1. Choose the Right Operator","text":"<ul> <li><code>sem_map</code>: Use for 1:1 transformations (each input \u2192 one output)</li> <li><code>sem_filter</code>: Use for selecting subsets based on conditions</li> <li><code>sem_agg</code>: Use for many:1 transformations (all inputs \u2192 one summary)</li> </ul>"},{"location":"semantic_operators/#2-write-clear-instructions","title":"2. Write Clear Instructions","text":"<pre><code># \u274c Vague\ninstructions = \"Process the data\"\n\n# \u2705 Clear and specific\ninstructions = \"\"\"\nExtract the product name, price, and category from each description.\nNormalize prices to USD. Categorize products as: Electronics, Clothing, or Home Goods.\n\"\"\"\n</code></pre>"},{"location":"semantic_operators/#3-use-appropriate-target-types","title":"3. Use Appropriate Target Types","text":"<pre><code># For simple extractions, use string types\nsem_map(\n    ...\n    target_type= \"category_name\"\n    ...\n)\n\n\n# For structured outputs, use Pydantic models\nclass Product(BaseModel):\n    name: str\n    price: float\n    category: str\n\nsem_map(\n    ...\n    target_type= Product\n    ...\n)\n</code></pre>"},{"location":"semantic_operators/#4-batch-processing","title":"4. Batch Processing","text":"<pre><code># Configure batch size for large datasets\nresult = await sem_map(\n    source=large_df,\n    target_type=MyType,\n    instructions=\"...\",\n    amap_batch_size=50  # Process 50 records at a time\n)\n</code></pre>"},{"location":"semantic_operators/#5-handle-both-ag-and-dataframe","title":"5. Handle Both AG and DataFrame","text":"<pre><code># Operators work with both types\ndf_result = await sem_filter(df, \"condition\")  # Returns DataFrame\nag_result = await sem_filter(ag, \"condition\")  # Returns AG\n</code></pre>"},{"location":"semantic_operators/#performance-considerations","title":"Performance Considerations","text":""},{"location":"semantic_operators/#batching","title":"Batching","text":"<p>Semantic operators support batching for efficient processing of large datasets:</p> <pre><code>result = await sem_map(\n    source=df,\n    target_type=MyType,\n    instructions=\"...\",\n    amap_batch_size=20  # Default for sem_filter\n)\n</code></pre>"},{"location":"semantic_operators/#integration-with-agentics-workflows","title":"Integration with Agentics Workflows","text":"<p>Semantic operators integrate seamlessly with other Agentics features:</p>"},{"location":"semantic_operators/#chaining-operations","title":"Chaining Operations","text":"<pre><code># Filter \u2192 Map \u2192 Aggregate pipeline\nfiltered = await sem_filter(df, \"High-value customers\")\nmapped = await sem_map(filtered, CustomerProfile, \"Extract profile details\")\nsummary = await sem_agg(mapped, Summary, \"Summarize customer segments\")\n</code></pre>"},{"location":"semantic_operators/#next","title":"Next","text":"<ul> <li>\ud83d\udc49 Semantic Operators Tutorial - Code examples</li> <li>\ud83d\udc49 Agentics (AG) for data modeling patterns and typed state containers</li> </ul>"},{"location":"semantic_operators/#go-to-index","title":"Go to Index","text":"<ul> <li>\ud83d\udc49 Index</li> </ul>"},{"location":"tool_integration/","title":"\ud83d\udd0c Tool Integration","text":"<p>Transducible functions can use external tools to enhance their capabilities through the Model Context Protocol (MCP). This enables LLM-powered workflows to access real-time data, execute code, query databases, and interact with external services.</p>"},{"location":"tool_integration/#what-are-tools","title":"What Are Tools?","text":"<p>Tools extend transducible functions with external capabilities:</p> <ul> <li>Web search - Retrieve real-time information from the internet</li> <li>Database queries - Access structured data from SQL/NoSQL databases</li> <li>API calls - Integrate with external services and APIs</li> <li>Code execution - Run computations and scripts</li> <li>File operations - Read/write files and process documents</li> <li>Custom functions - Any Python function you define</li> </ul>"},{"location":"tool_integration/#using-mcp-tools","title":"Using MCP Tools","text":"<p>MCP (Model Context Protocol) provides a standard way to expose tools to LLMs, making them discoverable and callable during transductions.</p>"},{"location":"tool_integration/#defining-an-mcp-server","title":"Defining an MCP Server","text":"<p>First, create an MCP server with your tools (see <code>examples/mcp_server_example.py</code>):</p> <pre><code>from ddgs import DDGS\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Search\")\n\n@mcp.tool()\ndef web_search(query: str, max_results: int = 5) -&gt; list[str]:\n    \"\"\"Search the web using DuckDuckGo.\n\n    Args:\n        query: Search query with optional operators\n        max_results: Number of results to return (5-20)\n\n    Returns:\n        List of search result snippets with titles and URLs\n    \"\"\"\n    results = DDGS().text(query, max_results=max_results)\n    return [f\"{r['title']}\\n{r['body']}\\n{r['href']}\" for r in results]\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n</code></pre>"},{"location":"tool_integration/#using-tools-in-transductions","title":"Using Tools in Transductions","text":"<p>There are several ways to import and use MCP tools in your transductions:</p>"},{"location":"tool_integration/#option-1-connect-to-remoteexternal-mcp-server","title":"Option 1: Connect to Remote/External MCP Server","text":"<p>Use <code>MCPServerAdapter</code> from <code>crewai_tools</code> to connect to a remote or external MCP server (e.g., a server provided by a third party or running on another machine):</p> <pre><code>import os\nfrom pydantic import BaseModel\nfrom typing import Optional\nfrom mcp import StdioServerParameters\nfrom crewai_tools import MCPServerAdapter\nfrom agentics.core.transducible_functions import transducible, Transduce\n\nclass ResearchQuery(BaseModel):\n    topic: Optional[str] = None\n    focus_area: Optional[str] = None\n\nclass ResearchReport(BaseModel):\n    summary: Optional[str] = None\n    key_findings: Optional[list[str]] = None\n    sources: Optional[list[str]] = None\n\n# Configure connection to remote MCP server\n# The server could be:\n# - A third-party MCP server (e.g., from a service provider)\n# - An MCP server running on another machine\n# - A pre-existing MCP server script\nserver_params = StdioServerParameters(\n    command=\"python3\",\n    args=[\"path/to/remote/mcp_server.py\"],  # Path to the MCP server\n    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n)\n\n# Connect to the remote MCP server\nwith MCPServerAdapter(server_params) as server_tools:\n    print(f\"Available tools from remote server: {[tool.name for tool in server_tools]}\")\n\n    @transducible(\n        tools=server_tools,  # Use tools from remote MCP server\n        reasoning=True,\n        max_iter=5\n    )\n    async def research_topic(state: ResearchQuery) -&gt; ResearchReport:\n        \"\"\"Research a topic using tools from remote MCP server.\"\"\"\n        return Transduce(state)\n\n    # Execute\n    query = ResearchQuery(\n        topic=\"Agentics framework\",\n        focus_area=\"practical applications\"\n    )\n    report = await research_topic(query)\n</code></pre> <p>Example: Connecting to a Third-Party MCP Server</p> <pre><code># Example: Connect to a hypothetical weather MCP server\nweather_server_params = StdioServerParameters(\n    command=\"npx\",  # MCP servers can be in any language\n    args=[\"-y\", \"@weather/mcp-server\"],  # npm package\n)\n\nwith MCPServerAdapter(weather_server_params) as weather_tools:\n    print(f\"Weather tools: {[tool.name for tool in weather_tools]}\")\n\n    @transducible(tools=weather_tools, reasoning=True)\n    async def get_weather_report(location: Location) -&gt; WeatherReport:\n        return Transduce(location)\n</code></pre> <p>Note: The <code>MCPServerAdapter</code> handles the connection lifecycle - it starts the server process when entering the context and stops it when exiting. You don't manage the server manually.</p>"},{"location":"tool_integration/#option-2-use-crewai-tools","title":"Option 2: Use CrewAI Tools","text":"<p>Agentics supports CrewAI tools, which provide a standardized interface for tool integration:</p> <pre><code>from pydantic import BaseModel\nfrom typing import Optional\nfrom agentics.core.transducible_functions import transducible, Transduce\nfrom crewai_tools import tool\n\nclass ResearchQuery(BaseModel):\n    topic: Optional[str] = None\n    focus_area: Optional[str] = None\n\nclass ResearchReport(BaseModel):\n    summary: Optional[str] = None\n    key_findings: Optional[list[str]] = None\n    sources: Optional[list[str]] = None\n\n# Define CrewAI tool\n@tool(\"Web Search Tool\")\ndef web_search(query: str, max_results: int = 5) -&gt; list[str]:\n    \"\"\"Search the web using DuckDuckGo.\n\n    Args:\n        query: Search query\n        max_results: Number of results to return\n\n    Returns:\n        List of search results with title, snippet, and URL\n    \"\"\"\n    from ddgs import DDGS\n    results = DDGS().text(query, max_results=max_results)\n    return [f\"{r['title']}\\n{r['body']}\\n{r['href']}\" for r in results]\n\n@transducible(\n    tools=[web_search],  # Pass CrewAI tool\n    reasoning=True,\n    max_iter=5\n)\nasync def research_topic(state: ResearchQuery) -&gt; ResearchReport:\n    \"\"\"Research a topic using web search and synthesize findings.\"\"\"\n    return Transduce(state)\n\n# Use it\nquery = ResearchQuery(\n    topic=\"Agentics framework\",\n    focus_area=\"practical applications\"\n)\nreport = await research_topic(query)\n</code></pre> <p>You can also use pre-built CrewAI tools:</p> <pre><code>from crewai_tools import SerperDevTool, WebsiteSearchTool\n\n# Use existing CrewAI tools\nsearch_tool = SerperDevTool()\nwebsite_tool = WebsiteSearchTool()\n\n@transducible(\n    tools=[search_tool, website_tool],\n    reasoning=True,\n    max_iter=5\n)\nasync def research_with_multiple_tools(state: ResearchQuery) -&gt; ResearchReport:\n    \"\"\"Research using multiple CrewAI tools.\"\"\"\n    return Transduce(state)\n</code></pre>"},{"location":"tool_integration/#option-3-launch-mcp-server-as-subprocess","title":"Option 3: Launch MCP Server as Subprocess","text":"<p>You can launch the MCP server as a separate subprocess and connect to it. This approach starts the server automatically when needed:</p> <pre><code>from mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# Launch MCP server as subprocess\n# The server_params specify how to start the server process\nserver_params = StdioServerParameters(\n    command=\"python\",  # Command to run\n    args=[\"mcp_server.py\"],  # Server script to execute\n)\n\n# stdio_client launches the server and manages the connection\nasync with stdio_client(server_params) as (read, write):\n    async with ClientSession(read, write) as session:\n        # Initialize connection and discover available tools\n        await session.initialize()\n        tools = await session.list_tools()\n\n        # Use tools in transduction\n        @transducible(\n            tools=tools,\n            reasoning=True,\n            max_iter=5\n        )\n        async def research_topic(state: ResearchQuery) -&gt; ResearchReport:\n            return Transduce(state)\n\n        # Execute\n        query = ResearchQuery(topic=\"Agentics framework\")\n        report = await research_topic(query)\n\n# Server process is automatically terminated when exiting the context\n</code></pre> <p>When to use each option: - Option 1 (Remote MCP Server): Best for connecting to third-party or external MCP servers; use when you want to leverage existing MCP services or servers running elsewhere - Option 2 (CrewAI tools): Best for leveraging the CrewAI ecosystem and pre-built tools; simplest for getting started with existing tools - Option 3 (MCP subprocess with ClientSession): Lower-level approach for advanced use cases; gives more control over server lifecycle and communication; use for custom server management</p>"},{"location":"tool_integration/#tool-usage-patterns","title":"Tool Usage Patterns","text":""},{"location":"tool_integration/#pattern-1-information-retrieval","title":"Pattern 1: Information Retrieval","text":"<p>Enrich data with external sources:</p> <pre><code>@transducible(tools=[web_search, database_query])\nasync def enrich_data(state: BasicInfo) -&gt; EnrichedInfo:\n    \"\"\"Enrich basic info with external data sources.\"\"\"\n    return Transduce(state)\n</code></pre>"},{"location":"tool_integration/#pattern-2-verification","title":"Pattern 2: Verification","text":"<p>Verify claims against external sources:</p> <pre><code>@transducible(tools=[fact_checker, web_search])\nasync def verify_claims(state: Claims) -&gt; VerifiedClaims:\n    \"\"\"Verify claims against external sources.\"\"\"\n    return Transduce(state)\n</code></pre>"},{"location":"tool_integration/#pattern-3-multi-step-reasoning","title":"Pattern 3: Multi-Step Reasoning","text":"<p>Solve complex problems requiring multiple tools:</p> <pre><code>@transducible(\n    tools=[web_search, calculator, code_executor],\n    reasoning=True,\n    max_iter=10,\n    verbose_agent=True  # See tool calls\n)\nasync def solve_complex_problem(state: Problem) -&gt; Solution:\n    \"\"\"Solve problems requiring multiple tools and reasoning steps.\"\"\"\n    return Transduce(state)\n</code></pre>"},{"location":"tool_integration/#creating-custom-tools","title":"Creating Custom Tools","text":"<p>Define your own tools following the MCP pattern:</p> <pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"CustomTools\")\n\n@mcp.tool()\ndef calculate_metrics(data: dict) -&gt; dict:\n    \"\"\"Calculate statistical metrics from data.\n\n    Args:\n        data: Dictionary with numeric values\n\n    Returns:\n        Dictionary with mean, median, std, etc.\n    \"\"\"\n    import statistics\n    values = list(data.values())\n    return {\n        \"mean\": statistics.mean(values),\n        \"median\": statistics.median(values),\n        \"stdev\": statistics.stdev(values) if len(values) &gt; 1 else 0\n    }\n\n@mcp.tool()\ndef fetch_database_record(record_id: str) -&gt; dict:\n    \"\"\"Fetch a record from the database.\n\n    Args:\n        record_id: Unique identifier for the record\n\n    Returns:\n        Record data as dictionary\n    \"\"\"\n    # Your database logic here\n    return db.get(record_id)\n</code></pre>"},{"location":"tool_integration/#tool-configuration","title":"Tool Configuration","text":"<p>Control how tools are used in your transductions:</p>"},{"location":"tool_integration/#limiting-tool-usage","title":"Limiting Tool Usage","text":"<pre><code># Limit tool usage\n@transducible(\n    tools=[expensive_api_tool],\n    max_iter=3,  # Maximum 3 tool calls\n    timeout=120  # 2 minute timeout including tool calls\n)\nasync def controlled_tool_usage(state: Input) -&gt; Output:\n    return Transduce(state)\n</code></pre>"},{"location":"tool_integration/#verbose-tool-logging","title":"Verbose Tool Logging","text":"<pre><code># Verbose tool logging\n@transducible(\n    tools=[web_search],\n    verbose_agent=True,  # Log each tool call\n    provide_explanation=True  # Include tool usage in explanation\n)\nasync def logged_tool_usage(state: Input) -&gt; Output:\n    return Transduce(state)\n\nresult, explanation = await logged_tool_usage(input_data)\nprint(f\"Tools used: {explanation.tools_called}\")\n</code></pre>"},{"location":"tool_integration/#best-practices","title":"Best Practices","text":"<ol> <li>Provide clear tool descriptions - Help the LLM understand when to use each tool</li> <li>Limit tool iterations - Prevent infinite loops with <code>max_iter</code></li> <li>Handle tool failures gracefully - Tools may timeout or return errors</li> <li>Use reasoning mode - Enable <code>reasoning=True</code> for complex tool orchestration</li> <li>Monitor tool usage - Use <code>verbose_agent=True</code> during development</li> <li>Cache tool results - Avoid redundant API calls for the same queries</li> </ol>"},{"location":"tool_integration/#example-robust-tool-usage","title":"Example: Robust Tool Usage","text":"<pre><code>@transducible(\n    tools=[web_search, database_query],\n    reasoning=True,\n    max_iter=5,\n    timeout=300,\n    verbose_agent=True,\n    provide_explanation=True\n)\nasync def robust_research(state: Query) -&gt; Report:\n    \"\"\"Research with fallback strategies if tools fail.\"\"\"\n    return Transduce(state)\n\ntry:\n    report, explanation = await robust_research(query)\n    print(f\"Used tools: {explanation.tools_called}\")\n    print(f\"Tool call count: {len(explanation.tool_calls)}\")\nexcept TimeoutError:\n    print(\"Research timed out - try simpler query or increase timeout\")\n</code></pre>"},{"location":"tool_integration/#see-also","title":"See Also","text":"<ul> <li>\ud83d\udc49 Transducible Functions - Core concepts and basic usage</li> <li>\ud83d\udc49 Optimization - Performance tuning and batch processing</li> <li>\ud83d\udc49 Examples - Complete MCP server example</li> <li>\ud83d\udc49 Index</li> </ul>"},{"location":"transducible_functions/","title":"\u2699\ufe0f Transducible Functions","text":"<p>Transducible functions are the workhorse of Agentics. They turn \u201ccall this LLM with a prompt\u201d into:</p> <p>A typed, explainable transformation <code>T: X \u2192 Y</code> with explanation about how each output field was produced.</p> <p>This document explains what transducible functions are, how they work in Agentics, and how to use them in practice \u2014 including dynamic generation and compositional patterns using the <code>&lt;&lt;</code> operator.</p>"},{"location":"transducible_functions/#1-what-is-a-transducible-function","title":"1. What Is a Transducible Function?","text":"<p>Formally, a transducible function <code>T: X \u2192 Y</code> is an explainable function that satisfies:</p> <ol> <li> <p>Local Evidence    Each output slot y\u1d62 is computed only from its evidence subset E\u1d62(x)*.  </p> <p>No field is generated \u201cfrom nowhere\u201d: if <code>subject</code> appears in the output, we know which inputs and instructions it depended on.</p> </li> <li> <p>Slot-Level Provenance     The mapping between input and output slots is explicit: T(y\u1d62) = E\u1d62</p> </li> </ol> <p>This induces a bipartite graph between input slots and output slots, which acts as the explainability trace of the transduction.</p> <p>Intuitively:</p> <ul> <li>An ordinary function only tells you \u201chere is the output.\u201d </li> <li>A transducible function also tells you \u201chere is the output, and here is exactly which inputs I used and why\u201d</li> </ul> <p>Transducible functions extend normal functions with structural transparency at the slot level.</p>"},{"location":"transducible_functions/#2-source-and-target-types","title":"2. Source and Target Types \ud83d\udcd0","text":"<p>Agentics uses Pydantic models to represent the input type <code>X</code> and the output type <code>Y</code>.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass UserMessage(BaseModel):\n    content: Optional[str] = None\n\nclass Email(BaseModel):\n    \"\"\"A simple email schema.\"\"\"\n    to: Optional[str] = Field(None, description=\"Recipient name or email address.\")\n    subject: Optional[str] = None\n    body: Optional[str] = None\n</code></pre> <ul> <li><code>UserMessage</code> is our Source type (<code>X</code>).</li> <li><code>Email</code> is our Target type (<code>Y</code>).</li> </ul> <p>Recommendation In transduction scenarios, it is often useful to declare fields as <code>Optional[...] = None</code>. This gives an LLM the ability to say \u201cI don\u2019t have enough evidence for this field\u201d by leaving it <code>null</code>, instead of hallucinating content.</p> <p>The transducible function we will define next will transform exactly one <code>UserMessage</code> into one <code>Email</code> (and later, we\u2019ll see how to scale to lists).</p>"},{"location":"transducible_functions/#3-defining-transducible-functions","title":"3. Defining Transducible Functions","text":"<p>In Agentics, transducible functions are <code>async</code> Python functions that:</p> <ul> <li>Accept exactly one instance of the source type <code>X</code> as input.</li> <li>Return exactly one instance of the target type <code>Y</code>.</li> </ul> <p>They can be defined in two main ways:</p> <ol> <li>Using the <code>@transducible()</code> decorator on an async Python function.</li> <li>Dynamically generating them from source and target types (e.g., via builders or the <code>&lt;&lt;</code> operator), with instructions and parameters.</li> </ol>"},{"location":"transducible_functions/#4-the-transducible-decorator","title":"4. The <code>@transducible()</code> Decorator","text":"<p>This section starts with the decorator pattern and then moves to dynamic generation and composition. The decorator turns an ordinary async function into a transducible function. When decorated with <code>@transducible()</code>, your function can return either:</p> <ul> <li>A concrete instance of the target type <code>Y</code> (pure Python logic), or</li> <li>A special <code>Transduce</code> object wrapping an instance of the source type <code>X</code>, which means:</li> </ul> <p>\u201cSend this source state to the LLM and let the model generate the target type <code>Y</code>.\u201d</p>"},{"location":"transducible_functions/#41-example-hybrid-llm-programmatic-logic","title":"4.1 Example: Hybrid LLM + Programmatic Logic","text":"<pre><code>import re\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom agentics.core.transducible_functions import transducible, Transduce\n\nclass UserMessage(BaseModel):\n    content: Optional[str] = None\n\nclass Email(BaseModel):\n    to: Optional[str] = None\n    subject: Optional[str] = None\n    body: Optional[str] = None\n</code></pre>"},{"location":"transducible_functions/#llm-driven-email-generation","title":"LLM-driven email generation","text":"<pre><code>@transducible()\nasync def write_email_with_llm(state: UserMessage) -&gt; Email:\n    \"\"\"Write a full email about the provided content.\n    The LLM is allowed to elaborate and make up reasonable details.\"\"\"\n    # Optionally mutate or pre-process state here\n    return Transduce(state)\n</code></pre> <p>Here, <code>Transduce(state)</code> signals:</p> <ul> <li>\u201cUse the transduction engine with this <code>UserMessage</code> as evidence.\u201d</li> <li>The LLM will generate an <code>Email</code> instance, respecting the schema.</li> </ul>"},{"location":"transducible_functions/#programmatic-email-extraction-no-llm","title":"Programmatic email extraction (no LLM)","text":"<pre><code>@transducible()\nasync def write_email_programmatically(state: UserMessage) -&gt; Email:\n    pattern = r\"^(Hi|Dear|Hello|Hey)\\s+([^,]+),\\s*(.+)$\"\n    match = re.match(pattern, state.content or \"\")\n    if match:\n        greeting, name, body = match.groups()\n        return Email(to=name, body=body)\n    # Not enough evidence \u2192 return an empty Email\n    return Email()\n</code></pre> <p>This function is also transducible, even if it does not call any LLM:</p> <ul> <li>It still respects totality: for any <code>UserMessage</code> it returns a valid <code>Email</code>.</li> <li>Local evidence is explicit: <code>to</code> and <code>body</code> come directly from <code>content</code>.</li> <li>Slot-level provenance is trivial: each field maps to a substring in <code>content</code>.</li> </ul> <p>Because both functions are transducible, they can be composed, traced, and plugged into Map\u2013Reduce pipelines in exactly the same way.</p>"},{"location":"transducible_functions/#5-executing-transducible-functions","title":"5. Executing Transducible Functions","text":"<p>You call a transducible function just like any other async function:</p> <pre><code>message = UserMessage(\n    content=\"Hi Lisa, I made great progress with the new release of Agentics 2.0\"\n)\n\ntarget1 = await write_email_with_llm(message)\ntarget2 = await write_email_programmatically(message)\n</code></pre>"},{"location":"transducible_functions/#51-example-outputs","title":"5.1 Example Outputs","text":"<p><code>target1</code> (LLM-based) may return something like:</p> <pre><code>{\n  \"to\": \"Lisa\",\n  \"subject\": \"Update on Agentics 2.0\",\n  \"body\": \"Hi Lisa,\\n\\nI wanted to share some exciting news about the new release of Agentics 2.0. Over the past week, I made great progress on the features we discussed...\\n\\nBest regards,\\n[Your Name]\"\n}\n</code></pre> <p><code>target2</code> (programmatic) will deterministically return:</p> <pre><code>{\n  \"to\": \"Lisa\",\n  \"subject\": null,\n  \"body\": \"I made great progress with the new release of Agentics 2.0\"\n}\n</code></pre> <p>A few important observations:</p> <ul> <li>The LLM output is stochastic: repeated calls may differ in style, but must remain logically transducible and semantically aligned with the evidence.</li> <li>The programmatic output is deterministic and brittle (it depends strictly on the regex).</li> <li>In practice, you combine both patterns:</li> <li>Use deterministic logic when the pattern is simple and strict.</li> <li>Use LLM-based transduction when structure is fixed but content is open-ended.</li> </ul>"},{"location":"transducible_functions/#6-dynamic-generation-composition-of-transducible-functions","title":"6. Dynamic Generation &amp; Composition of Transducible Functions","text":"<p>Beyond the decorator, Agentics lets you generate and compose transducible functions dynamically using the <code>&lt;&lt;</code> operator and helpers such as <code>With(...)</code>.</p> <p>Conceptually, the operator implements:</p> <p>Typed transduction construction <code>Y &lt;&lt; X</code> means: \u201cBuild a transducible function that maps from type <code>X</code> to type <code>Y</code>.\u201d </p> <p>You can use it with:</p> <ul> <li>Types (<code>Y &lt;&lt; X</code>),</li> <li>Existing transducible functions (<code>Y &lt;&lt; f</code>), and</li> <li>Configuration wrappers (<code>Y &lt;&lt; With(X, ...)</code>).</li> </ul>"},{"location":"transducible_functions/#61-minimal-setup","title":"6.1 Minimal Setup","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\nfrom agentics.core.transducible_functions import With\n\nclass GenericInput(BaseModel):\n    content: Optional[str] = None\n\nclass Email(BaseModel):\n    \"\"\"Email generated from a generic input.\"\"\"\n    to: Optional[str] = Field(None, description=\"Recipient of the email.\")\n    subject: Optional[str] = None\n    body: Optional[str] = None\n</code></pre>"},{"location":"transducible_functions/#62-dynamic-generation-with-type-function","title":"6.2 Dynamic Generation with <code>&lt;&lt;</code> (Type \u2192 Function)","text":"<p>The simplest form of dynamic generation is:</p> <pre><code>write_mail = Email &lt;&lt; GenericInput\n</code></pre> <p>This constructs a transducible function:</p> <pre><code>write_mail: GenericInput \u2192 Email\n</code></pre> <p>Usage:</p> <pre><code>input_state = GenericInput(\n    content=\"Write a news story on the winner of Super Bowl in 2025 and send it to Alfio.\"\n)\n\nmail = await write_mail(input_state)\nprint(mail.model_dump_json(indent=2))\n</code></pre> <p>Here, <code>Email &lt;&lt; GenericInput</code> tells Agentics:</p> <ul> <li>\u201cCreate an LLM-backed transducible function that maps a <code>GenericInput</code> into an <code>Email</code>.\u201d</li> <li>The default instructions depend on your configuration and global defaults (or you can refine them via <code>With</code>, shown below).</li> </ul>"},{"location":"transducible_functions/#63-composing-transductions-with","title":"6.3 Composing Transductions with <code>&lt;&lt;</code>","text":"<p>You can build multi-step pipelines by composing transducible functions and types using <code>&lt;&lt;</code>.</p> <p>Suppose we want to add a summary step on top of the email:</p> <pre><code>class Summary(BaseModel):\n    summary_text: Optional[str] = None\n</code></pre>"},{"location":"transducible_functions/#631-two-step-composition","title":"6.3.1. Two-step composition","text":"<pre><code>input_state = GenericInput(\n    content=\"Write a news story on the winner of Super Bowl in 2025 and send it to Alfio.\"\n)\n\nwrite_mail = Email &lt;&lt; GenericInput             # GenericInput \u2192 Email\nsummary_from_email = Summary &lt;&lt; Email          # Email \u2192 Summary\n\n# Composition by function application\nmail = await write_mail(input_state)\nsummary = await summary_from_email(mail)\n\nprint(mail.model_dump_json(indent=2))\nprint(summary.model_dump_json(indent=2))\n</code></pre>"},{"location":"transducible_functions/#632-composition-via-on-functions","title":"6.3.2. Composition via <code>&lt;&lt;</code> on functions","text":"<p>You can also let <code>&lt;&lt;</code> perform the composition directly:</p> <pre><code># Compose Summary on top of write_mail\nsummary_composite_1 = Summary &lt;&lt; write_mail   # GenericInput \u2192 Summary\n\nsummary1 = await summary_composite_1(input_state)\nprint(summary1.model_dump_json(indent=2))\n</code></pre> <p>Or inline:</p> <pre><code>summary_composite_2 = Summary &lt;&lt; (Email &lt;&lt; GenericInput)\nsummary2 = await summary_composite_2(input_state)\nprint(summary2.model_dump_json(indent=2))\n</code></pre> <p>In all cases, the pipeline is:</p> <pre><code>GenericInput  \u2192  Email  \u2192  Summary\n</code></pre> <p>but you can choose whether to:</p> <ul> <li>Write the steps explicitly, or</li> <li>Build them into a single composed transducible function.</li> </ul>"},{"location":"transducible_functions/#64-using-with-for-configured-dynamic-transduction","title":"6.4 Using <code>With(...)</code> for Configured Dynamic Transduction","text":"<p>The <code>With(...)</code> helper lets you attach instructions and options to dynamic transductions.</p> <p>Example: first generate an email, then rewrite it into a compact summary:</p> <pre><code>from agentics.core.transducible_functions import With\n\nclass Summary(BaseModel):\n    summary_text: Optional[str] = None\n\n# A basic dynamic transduction\nwrite_mail = Email &lt;&lt; GenericInput\n\n# A configured transduction: Email \u2192 Summary\nsummarize = Summary &lt;&lt; With(\n    Email,\n    instructions=\"Rewrite the email into a concise summary.\",\n    enforce_output_type=True,\n    verbose_transduction=False,\n)\n\ninput_state = GenericInput(\n    content=\"Philadelphia Eagles won Super Bowl 2025. Draft a message to the press list.\"\n)\n\nmail = await write_mail(input_state)\nsummary = await summarize(mail)\n\nprint(mail.model_dump_json(indent=2))\nprint(summary.model_dump_json(indent=2))\n</code></pre> <p>Here:</p> <ul> <li><code>With(Email, ...)</code> tells Agentics: \u201cWhen you see an <code>Email</code> as input, apply these instructions and guarantees to produce a <code>Summary</code>.\u201d</li> <li><code>enforce_output_type=True</code> strengthens validation so outputs must conform to <code>Summary</code>.</li> <li><code>verbose_transduction=False</code> keeps logs / metadata minimal (implementation-dependent).</li> </ul> <p>Because <code>Summary &lt;&lt; With(Email, ...)</code> is still a transducible function, you can compose it further, call it on lists, or plug it into Map\u2013Reduce.</p>"},{"location":"transducible_functions/#65-adding-explanations-with-with","title":"6.5. Adding explanations with <code>With(...)</code>","text":"<p>You can also ask for a structured explanation of the classification:</p> <pre><code>classify_genre = Genre &lt;&lt; With(\n    Movie,\n    provide_explanation=True,\n)\n\ngenre, explanation = await classify_genre(movie)\nprint(genre.model_dump_json(indent=2))\nprint(explanation.model_dump_json(indent=2))\n</code></pre> <p>Here, <code>provide_explanation=True</code> configures the dynamic transduction so that:</p> <ul> <li>The first output is the typed <code>Genre</code>.</li> <li>The second output is an explanation object (typically another Pydantic model),   capturing why the classifier picked that genre.</li> </ul> <p>This pattern generalizes:</p> <ul> <li><code>With(..., provide_explanation=True)</code> can be used with other source/target pairs.</li> <li>Explanations can be logged, inspected, or surfaced in UI as transparent justification for the model\u2019s decision.</li> </ul>"},{"location":"transducible_functions/#65-with-function-reference","title":"6.5 <code>With()</code> Function Reference","text":"<p>The <code>With()</code> function creates a <code>TransductionConfig</code> object that wraps a source model with configuration parameters. It's used with the <code>&lt;&lt;</code> operator to create configured transducible functions dynamically.</p> <p>Signature: <pre><code>def With(model: Type[BaseModel], **kwargs) -&gt; TransductionConfig\n</code></pre></p> <p>Parameters:</p> <p>All parameters from the <code>@transducible()</code> decorator are supported:</p> Parameter Type Default Description <code>instructions</code> <code>str</code> <code>\"\"</code> Custom instructions for the LLM on how to perform the transduction <code>tools</code> <code>list[Any]</code> <code>[]</code> List of tools (MCP, CrewAI, or LangChain) available during transduction <code>enforce_output_type</code> <code>bool</code> <code>False</code> If <code>True</code>, raises <code>TypeError</code> if output doesn't match target type <code>llm</code> <code>Any</code> <code>AG.get_llm_provider()</code> LLM provider to use (OpenAI, WatsonX, Ollama, etc.) <code>reasoning</code> <code>bool</code> <code>False</code> Enable reasoning mode for complex transductions <code>max_iter</code> <code>int</code> <code>10</code> Maximum iterations for agentic reasoning loops <code>verbose_transduction</code> <code>bool</code> <code>True</code> Print detailed transduction logs <code>verbose_agent</code> <code>bool</code> <code>False</code> Print agent-level execution logs <code>batch_size</code> <code>int</code> <code>10</code> Number of items to process in parallel batches <code>provide_explanation</code> <code>bool</code> <code>False</code> Return explanation alongside result (see Section 6.6) <code>timeout</code> <code>int</code> <code>300</code> Timeout in seconds for each transduction <code>post_processing_function</code> <code>Callable</code> <code>None</code> Function to apply to outputs after transduction <code>persist_output</code> <code>str</code> <code>None</code> Path to save intermediate batch results <code>transduce_fields</code> <code>list[str]</code> <code>None</code> Specific fields to use for transduction <code>prompt_template</code> <code>str</code> <code>None</code> Custom prompt template for the LLM <code>areduce</code> <code>bool</code> <code>False</code> Use reduce mode instead of map (for aggregations) <p>Usage Patterns:</p> <pre><code># Basic usage with instructions\nclassify = Genre &lt;&lt; With(Movie, instructions=\"Classify the movie genre\")\n\n# Multiple parameters\nenrich = EnrichedData &lt;&lt; With(\n    RawData,\n    instructions=\"Enrich with external data\",\n    tools=[web_search_tool],\n    batch_size=20,\n    timeout=600,\n    provide_explanation=True\n)\n\n# Comparison: With() vs @transducible()\n# These are equivalent:\n\n# Using With()\nfn1 = TargetType &lt;&lt; With(SourceType, instructions=\"Transform data\")\n\n# Using decorator\n@transducible(instructions=\"Transform data\")\nasync def fn2(state: SourceType) -&gt; TargetType:\n    return Transduce(state)\n</code></pre> <p>When to use <code>With()</code> vs <code>@transducible()</code>:</p> <ul> <li>Use <code>With()</code> for dynamic, one-off transductions where you don't need a named function</li> <li>Use <code>@transducible()</code> for reusable functions that you'll call multiple times or compose into larger workflows</li> <li><code>With()</code> is ideal for exploratory work and inline transformations</li> <li><code>@transducible()</code> is better for production code with clear function names and documentation</li> </ul>"},{"location":"transducible_functions/#66-result-unpacking-with-transductionresult","title":"6.6 Result Unpacking with <code>TransductionResult</code>","text":"<p>When you set <code>provide_explanation=True</code> (either in <code>@transducible()</code> or <code>With()</code>), the transduction returns a <code>TransductionResult</code> object that supports automatic unpacking.</p> <p>The <code>TransductionResult</code> Class:</p> <pre><code>class TransductionResult:\n    def __init__(self, value, explanation):\n        self.value = value          # The actual transduced output\n        self.explanation = explanation  # Explanation of how it was derived\n\n    def __iter__(self):\n        yield self.value\n        yield self.explanation\n</code></pre> <p>Automatic Unpacking Behavior:</p> <p>The framework automatically detects how you assign the result:</p> <pre><code># Single assignment - get only the value\nresult = await classify_genre(movie)\nprint(result.genre)  # Access the Genre object directly\n\n# Tuple unpacking - get both value and explanation\ngenre, explanation = await classify_genre(movie)\nprint(genre.genre)  # The Genre object\nprint(explanation.reasoning)  # The explanation object\n</code></pre> <p>Example with Decorator:</p> <pre><code>@transducible(provide_explanation=True)\nasync def classify_genre(state: Movie) -&gt; Genre:\n    \"\"\"Classify the genre of the source Movie.\"\"\"\n    return Transduce(state)\n\nmovie = Movie(\n    movie_name=\"The Godfather\",\n    description=\"Crime family drama\",\n    year=1972\n)\n\n# Get both result and explanation\ngenre, explanation = await classify_genre(movie)\n\nprint(f\"Genre: {genre.genre}\")\nprint(f\"Reasoning: {explanation.reasoning}\")\nprint(f\"Confidence: {explanation.confidence}\")\n</code></pre> <p>Example with <code>With()</code>:</p> <pre><code>classify_genre = Genre &lt;&lt; With(\n    Movie,\n    provide_explanation=True,\n    instructions=\"Classify based on plot and themes\"\n)\n\n# Tuple unpacking works the same way\ngenre, explanation = await classify_genre(movie)\n</code></pre> <p>Batch Processing with Explanations:</p> <p>When processing lists, each item gets its own explanation:</p> <pre><code>movies = [movie1, movie2, movie3]\n\n# Returns list of values and list of explanations\ngenres, explanations = await classify_genre(movies)\n\nfor genre, explanation in zip(genres, explanations):\n    print(f\"{genre.genre}: {explanation.reasoning}\")\n</code></pre> <p>Note: If you don't need explanations, simply omit <code>provide_explanation=True</code> and the function returns only the transduced value(s).</p>"},{"location":"transducible_functions/#7-batch-processing-with-lists","title":"7. Batch Processing with Lists","text":"<p>Transducible functions automatically support batch processing. When you pass a list of items, they are processed efficiently:</p> <pre><code>messages = [\n    UserMessage(content=\"Hi John, I made great progress with Agentics.\"),\n    UserMessage(content=\"Hi, I fixed the last blocking bug in the pipeline.\"),\n]\n\n# Automatically processes all messages\nemails = await write_email_with_llm(messages)\n</code></pre> <p>For detailed information on Map-Reduce operations, scaling to large datasets, and aggregation patterns, see the dedicated Map-Reduce documentation.</p>"},{"location":"transducible_functions/#8-evidence-provenance-and-explainability","title":"8. Evidence, Provenance, and Explainability","text":"<p>Because transducible functions are defined over explicit types and carry evidence subsets, Agentics can:</p> <ul> <li>Track which input fields contributed to the output.</li> <li>Attach this trace as metadata to your states (depending on your Agentics configuration).</li> </ul> <p>For example, in the email examples:</p> <ul> <li><code>Email.to</code> is mapped to (a span inside) <code>UserMessage.content</code>.</li> <li><code>Email.subject</code> may depend on the entire <code>content</code>.</li> <li><code>Email.body</code> is mostly grounded in <code>content</code>, plus stylistic priors from instructions.</li> </ul> <p>This is critical when you:</p> <ul> <li>Need auditable LLM behavior.</li> <li>Want to debug why a particular field was generated.</li> <li>Need to enforce \"no hallucination from outside these inputs\" policies.</li> </ul>"},{"location":"transducible_functions/#9-when-to-create-a-new-transducible-function","title":"9. When to Create a New Transducible Function","text":"<p>In a real system, you'll typically end up with many small, focused transducible functions instead of one giant one.</p> <p>Good reasons to define a separate transducible function:</p> <ul> <li>You're doing a logically distinct step:</li> <li>e.g., extract entities, normalize names, classify intent, summarize conversation.</li> <li>You want to test and benchmark that step independently.</li> <li>You expect to reuse it across pipelines.</li> <li>You need different instructions, constraints, or safety properties for that stage.</li> </ul>"},{"location":"transducible_functions/#10-summary","title":"10. Summary \u2705","text":"<ul> <li>A transducible function is a typed, explainable mapping <code>T: X \u2192 Y</code> with:</li> <li>Totality, Local Evidence, and Slot-Level Provenance.</li> <li>In Agentics:</li> <li>Inputs and outputs are modeled as Pydantic types (<code>X</code>, <code>Y</code>).</li> <li>You can define transducible functions via:<ul> <li>The <code>@transducible()</code> decorator,</li> <li>Dynamic builders like <code>make_transducible_function</code>, and</li> <li>The <code>&lt;&lt;</code> operator (with or without <code>With(...)</code>).</li> </ul> </li> <li>Functions can be purely programmatic, purely LLM-based, or hybrid.</li> <li>Transducible functions:</li> <li>Scale from single calls to batch Map\u2013Reduce workloads.</li> <li>Expose structured explainability traces for each output field.</li> <li>Compose into robust, interpretable, large-scale reasoning pipelines.</li> </ul>"},{"location":"transducible_functions/#next","title":"Next","text":"<ul> <li>\ud83d\udc49 Transducible Functions Tutorial to see how transducible functions works in practice</li> <li>\ud83d\udc49 Map-Reduce Operations - Scaling with map and reduce, batch processing patterns</li> </ul>"},{"location":"transducible_functions/#go-to-index","title":"Go to Index","text":"<ul> <li>\ud83d\udc49 Index</li> </ul>"}]}