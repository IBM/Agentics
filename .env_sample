################################################################################
# AGENTICS LLM CONFIGURATION
################################################################################
# This file contains all the environment variables needed to configure LLM
# providers for use with Agentics. All LLM providers are optional - configure
# only the ones you need. The system will auto-discover which LLMs are available
# based on the environment variables present.
################################################################################


################################################################################
# GEMINI (Optional)
################################################################################
GEMINI_API_KEY=
GEMINI_MODEL_ID="gemini/gemini-2.0-flash"


################################################################################
# OPENAI (Optional)
################################################################################
OPENAI_API_KEY=
OPENAI_MODEL_ID="openai/gpt-4"


################################################################################
# OPENAI-COMPATIBLE LLMs (Optional)
#
# Configure OpenAI-compatible API providers with custom prefixes.
#
# Supported patterns:
# - Default variant: OPENAI_COMPATIBLE_*
# - Custom variants: OPENAI_COMPATIBLE_<VARIANT>_*
#
# Each variant requires three variables:
# - <PREFIX>_API_KEY: Your API key
# - <PREFIX>_MODEL_ID: The model identifier
# - <PREFIX>_BASE_URL: The base URL of the API
#
# Examples:
################################################################################

# Default OpenAI-compatible variant
OPENAI_COMPATIBLE_API_KEY=
OPENAI_COMPATIBLE_MODEL_ID=
OPENAI_COMPATIBLE_BASE_URL=

# Claude via Anthropic API (example custom variant)
# OPENAI_COMPATIBLE_CLAUDE_API_KEY=
# OPENAI_COMPATIBLE_CLAUDE_MODEL_ID="claude-3-sonnet-20240229"
# OPENAI_COMPATIBLE_CLAUDE_BASE_URL="https://api.anthropic.com"

# Mistral via Mistral API (example custom variant)
# OPENAI_COMPATIBLE_MISTRAL_API_KEY=
# OPENAI_COMPATIBLE_MISTRAL_MODEL_ID="mistral-large"
# OPENAI_COMPATIBLE_MISTRAL_BASE_URL="https://api.mistral.ai/v1"

# Groq API (example custom variant)
# OPENAI_COMPATIBLE_GROQ_API_KEY=
# OPENAI_COMPATIBLE_GROQ_MODEL_ID="mixtral-8x7b-32768"
# OPENAI_COMPATIBLE_GROQ_BASE_URL="https://api.groq.com/openai/v1"

# DeepSeek API (example custom variant)
# OPENAI_COMPATIBLE_DEEPSEEK_API_KEY=
# OPENAI_COMPATIBLE_DEEPSEEK_MODEL_ID="deepseek-chat"
# OPENAI_COMPATIBLE_DEEPSEEK_BASE_URL="https://api.deepseek.com"

# You can add any other OpenAI-compatible provider by following the pattern:
# OPENAI_COMPATIBLE_<YOUR_PROVIDER>_API_KEY=
# OPENAI_COMPATIBLE_<YOUR_PROVIDER>_MODEL_ID=
# OPENAI_COMPATIBLE_<YOUR_PROVIDER>_BASE_URL=


################################################################################
# WATSONX (Optional)
################################################################################
MODEL_ID=watsonx/meta-llama/llama-3-3-70b-instruct
WATSONX_URL=https://us-south.ml.cloud.ibm.com
WATSONX_APIKEY=
WATSONX_PROJECTID=


################################################################################
# VLLM - vLLM Server (Optional)
#
# vLLM is an open-source LLM serving framework.
# Configure if you have a vLLM server running locally or remotely.
################################################################################

# Required: vLLM server URL
VLLM_URL=http://localhost:8000/v1

# Optional: Specific model ID for vLLM
VLLM_MODEL_ID="meta-llama/Llama-3.3-70B-Instruct"


################################################################################
# OLLAMA (Optional)
#
# Ollama is an open-source LLM runner that can run models locally.
# Configure if you have Ollama installed and running.
################################################################################

# The model to use with Ollama
# OLLAMA_MODEL_ID="deepseek-r1:latest"
# OLLAMA_MODEL_ID="llama2:latest"
# OLLAMA_MODEL_ID="mistral:latest"


################################################################################
# CREWAI CONFIGURATION
################################################################################

CREWAI_DISABLE_TELEMETRY=true
CREWAI_DISABLE_TRACING=true
CREWAI_TELEMETRY=false
OTEL_SDK_DISABLED=true
CREWAI_TRACING_DISABLED=true
CREWAI_SILENT=true
AGENTICS_TRACE_MODE="off"


################################################################################
# USAGE NOTES
################################################################################
#
# 1. AUTO-DISCOVERY:
#    All configured LLMs are automatically discovered when the module loads.
#    In Python:
#      from agentics.core.llm_connections import available_llms
#      print(list(available_llms.keys()))
#
# 2. ACCESSING LLMs:
#    Get a specific LLM by name:
#      from agentics.core.llm_connections import available_llms
#      claude_llm = available_llms['openai_compatible_claude']
#      mistral_llm = available_llms['openai_compatible_mistral']
#
# 3. DEFAULT LLM:
#    Get the first available LLM:
#      from agentics.core.llm_connections import get_llm_provider
#      llm = get_llm_provider()
#
# 4. MULTIPLE VARIANTS:
#    You can configure multiple OpenAI-compatible providers at once.
#    Each will be automatically discovered and made available by its name.
#
# 5. ENVIRONMENT VARIABLE NAMING:
#    Variant names are converted to lowercase and underscores are preserved.
#    For example:
#      OPENAI_COMPATIBLE_CLAUDE_API_KEY -> openai_compatible_claude
#      OPENAI_COMPATIBLE_MISTRAL_API_KEY -> openai_compatible_mistral
#
################################################################################

################################################################################
# LITELLM (Optional) - 100+ LLM Providers via CrewAI
#
# CrewAI has native support for LiteLLM, which provides unified access to
# 100+ LLM providers. Simply configure the model using LiteLLM's format.
#
# CrewAI will automatically use the litellm/ prefix to route to LiteLLM.
#
# Supported providers include:
# - OpenAI: gpt-4, gpt-3.5-turbo, gpt-4-turbo, etc.
# - Anthropic: claude-3-opus, claude-3-sonnet, claude-3-haiku, etc.
# - Mistral: mistral-large, mistral-medium, mistral-small, etc.
# - Cohere: command-r, command-r-plus, command, etc.
# - Google: text-bison, gemini-pro, vertex_ai, etc.
# - Meta Llama: via Replicate, Together AI, and others
# - Replicate, Together AI, Hugging Face, AI21, and 50+ more providers
#
# Full list: https://docs.litellm.ai/docs/providers
# CrewAI docs: https://docs.crewai.com/en/learn/llm-connections#supported-providers
################################################################################

# Required: Model identifier in LiteLLM format
# Format: "provider/model-name" or just "model-name" for some providers
# Examples:
# - "gpt-4" (OpenAI)
# - "claude-3-sonnet-20240229" (Anthropic)
# - "mistral-large" (Mistral)
# - "command-r-plus" (Cohere)
# LITELLM_MODEL=

# Optional: Temperature (default: 0.8)
# LITELLM_TEMPERATURE=0.8

# Optional: Top P (default: 0.9)
# LITELLM_TOP_P=0.9

# Optional: Maximum tokens for responses
# LITELLM_MAX_TOKENS=2048
